
==> Audit <==
|-----------|----------------|----------|------|---------|---------------------|---------------------|
|  Command  |      Args      | Profile  | User | Version |     Start Time      |      End Time       |
|-----------|----------------|----------|------|---------|---------------------|---------------------|
| start     |                | minikube | saad | v1.33.0 | 08 May 24 14:04 PKT | 08 May 24 14:13 PKT |
| service   | flask-service  | minikube | saad | v1.33.0 | 08 May 24 15:30 PKT | 08 May 24 15:30 PKT |
| service   | flask-service  | minikube | saad | v1.33.0 | 08 May 24 15:30 PKT | 08 May 24 15:47 PKT |
| dashboard |                | minikube | saad | v1.33.0 | 08 May 24 15:35 PKT |                     |
| service   | qdrant-service | minikube | saad | v1.33.0 | 08 May 24 15:46 PKT |                     |
| service   | flask-service  | minikube | saad | v1.33.0 | 08 May 24 15:47 PKT | 08 May 24 15:58 PKT |
| service   | flask-service  | minikube | saad | v1.33.0 | 08 May 24 16:29 PKT | 08 May 24 16:31 PKT |
| service   | flask-service  | minikube | saad | v1.33.0 | 08 May 24 16:34 PKT | 08 May 24 16:36 PKT |
| service   | flask-service  | minikube | saad | v1.33.0 | 08 May 24 16:38 PKT | 09 May 24 03:01 PKT |
| service   | flask-service  | minikube | saad | v1.33.0 | 09 May 24 03:20 PKT |                     |
|-----------|----------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/05/08 14:04:36
Running on machine: 172-15-75-191
Binary: Built with gc go1.22.2 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0508 14:04:36.347778   71032 out.go:291] Setting OutFile to fd 1 ...
I0508 14:04:36.348267   71032 out.go:343] isatty.IsTerminal(1) = true
I0508 14:04:36.348270   71032 out.go:304] Setting ErrFile to fd 2...
I0508 14:04:36.348274   71032 out.go:343] isatty.IsTerminal(2) = true
I0508 14:04:36.348772   71032 root.go:338] Updating PATH: /Users/saad/.minikube/bin
W0508 14:04:36.349075   71032 root.go:314] Error reading config file at /Users/saad/.minikube/config/config.json: open /Users/saad/.minikube/config/config.json: no such file or directory
I0508 14:04:36.349877   71032 out.go:298] Setting JSON to false
I0508 14:04:36.383285   71032 start.go:129] hostinfo: {"hostname":"172-15-75-191.lightspeed.irvnca.sbcglobal.net","uptime":3190756,"bootTime":1711968320,"procs":391,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.4.1","kernelVersion":"23.4.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"6184c406-7b5b-5dae-be42-05d747ba4e1a"}
W0508 14:04:36.383377   71032 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0508 14:04:36.389137   71032 out.go:177] 😄  minikube v1.33.0 on Darwin 14.4.1 (arm64)
I0508 14:04:36.398166   71032 notify.go:220] Checking for updates...
I0508 14:04:36.398291   71032 driver.go:392] Setting default libvirt URI to qemu:///system
W0508 14:04:36.398295   71032 preload.go:294] Failed to list preload files: open /Users/saad/.minikube/cache/preloaded-tarball: no such file or directory
I0508 14:04:36.398531   71032 global.go:112] Querying for installed drivers using PATH=/Users/saad/.minikube/bin:/Users/saad/google-cloud-sdk/bin:/opt/homebrew/opt/mongodb-community@5.0/bin:/Library/Frameworks/Python.framework/Versions/3.10/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/Library/Frameworks/Python.framework/Versions/3.11/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin
I0508 14:04:36.399331   71032 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0508 14:04:36.399354   71032 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0508 14:04:36.399558   71032 global.go:133] hyperkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "hyperkit": executable file not found in $PATH Reason: Fix:Run 'brew install hyperkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/ Version:}
I0508 14:04:36.399626   71032 global.go:133] parallels default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "prlctl": executable file not found in $PATH Reason: Fix:Install Parallels Desktop for Mac Doc:https://minikube.sigs.k8s.io/docs/drivers/parallels/ Version:}
I0508 14:04:36.399855   71032 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-aarch64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0508 14:04:36.399952   71032 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0508 14:04:36.400010   71032 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0508 14:04:38.212912   71032 docker.go:122] docker version: linux-24.0.2:Docker Desktop 4.20.1 (110738)
I0508 14:04:38.213290   71032 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0508 14:04:39.437243   71032 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.223938s)
I0508 14:04:39.437907   71032 info.go:266] docker info: {ID:b9d3e1c8-05b0-4732-bbc4-ff4cfc66b785 Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:68 OomKillDisable:false NGoroutines:95 SystemTime:2024-05-08 09:04:39.395056592 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:5.15.49-linuxkit-pr OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8337125376 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/saad/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.5] map[Name:compose Path:/Users/saad/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.18.1] map[Name:dev Path:/Users/saad/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/saad/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.19] map[Name:init Path:/Users/saad/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.4] map[Name:sbom Path:/Users/saad/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/saad/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/saad/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:v0.12.0]] Warnings:<nil>}}
I0508 14:04:39.438026   71032 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0508 14:04:39.438039   71032 driver.go:314] not recommending "ssh" due to default: false
I0508 14:04:39.438052   71032 driver.go:349] Picked: docker
I0508 14:04:39.438057   71032 driver.go:350] Alternatives: [ssh]
I0508 14:04:39.438060   71032 driver.go:351] Rejects: [podman hyperkit parallels qemu2 virtualbox vmware]
I0508 14:04:39.443518   71032 out.go:177] ✨  Automatically selected the docker driver
I0508 14:04:39.451733   71032 start.go:297] selected driver: docker
I0508 14:04:39.451737   71032 start.go:901] validating driver "docker" against <nil>
I0508 14:04:39.451745   71032 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0508 14:04:39.451890   71032 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0508 14:04:39.601491   71032 info.go:266] docker info: {ID:b9d3e1c8-05b0-4732-bbc4-ff4cfc66b785 Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:68 OomKillDisable:false NGoroutines:95 SystemTime:2024-05-08 09:04:39.563633342 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:5.15.49-linuxkit-pr OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8337125376 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/saad/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.5] map[Name:compose Path:/Users/saad/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.18.1] map[Name:dev Path:/Users/saad/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/saad/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.19] map[Name:init Path:/Users/saad/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.4] map[Name:sbom Path:/Users/saad/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/saad/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/saad/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:v0.12.0]] Warnings:<nil>}}
I0508 14:04:39.601671   71032 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0508 14:04:39.601822   71032 start_flags.go:393] Using suggested 2200MB memory alloc based on sys=8192MB, container=7950MB
I0508 14:04:39.602621   71032 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0508 14:04:39.606607   71032 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0508 14:04:39.611147   71032 cni.go:84] Creating CNI manager for ""
I0508 14:04:39.611318   71032 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0508 14:04:39.611322   71032 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0508 14:04:39.611376   71032 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0508 14:04:39.614557   71032 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0508 14:04:39.622661   71032 cache.go:121] Beginning downloading kic base image for docker with docker
I0508 14:04:39.626546   71032 out.go:177] 🚜  Pulling base image v0.0.43 ...
I0508 14:04:39.634502   71032 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0508 14:04:39.634525   71032 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0508 14:04:39.700843   71032 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 to local cache
I0508 14:04:39.703282   71032 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local cache directory
I0508 14:04:39.703647   71032 image.go:118] Writing gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 to local cache
I0508 14:04:39.973062   71032 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4
I0508 14:04:39.973136   71032 cache.go:56] Caching tarball of preloaded images
I0508 14:04:39.974747   71032 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0508 14:04:39.984068   71032 out.go:177] 💾  Downloading Kubernetes v1.30.0 preload ...
I0508 14:04:39.985240   71032 preload.go:237] getting checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 ...
I0508 14:04:40.785139   71032 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4?checksum=md5:677034533668c42fec962cc52f9b3c42 -> /Users/saad/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4
I0508 14:10:36.857015   71032 preload.go:248] saving checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 ...
I0508 14:10:36.857282   71032 preload.go:255] verifying checksum of /Users/saad/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 ...
I0508 14:10:37.862363   71032 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0508 14:10:37.863216   71032 profile.go:143] Saving config to /Users/saad/.minikube/profiles/minikube/config.json ...
I0508 14:10:37.863458   71032 lock.go:35] WriteFile acquiring /Users/saad/.minikube/profiles/minikube/config.json: {Name:mk7361b1703625bb00c53aa837b1d59899613fc7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:11:53.639470   71032 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 as a tarball
I0508 14:11:53.639490   71032 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 from local cache
I0508 14:12:16.873926   71032 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 from cached tarball
I0508 14:12:16.875068   71032 cache.go:194] Successfully downloaded all kic artifacts
I0508 14:12:16.880361   71032 start.go:360] acquireMachinesLock for minikube: {Name:mkc3a4a534697f9364d67b79c0ed4276d4a2af84 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0508 14:12:16.886636   71032 start.go:364] duration metric: took 5.912709ms to acquireMachinesLock for "minikube"
I0508 14:12:16.887050   71032 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0508 14:12:16.887438   71032 start.go:125] createHost starting for "" (driver="docker")
I0508 14:12:16.900117   71032 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
I0508 14:12:16.904066   71032 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0508 14:12:16.904665   71032 client.go:168] LocalClient.Create starting
I0508 14:12:16.906409   71032 main.go:141] libmachine: Creating CA: /Users/saad/.minikube/certs/ca.pem
I0508 14:12:17.361006   71032 main.go:141] libmachine: Creating client certificate: /Users/saad/.minikube/certs/cert.pem
I0508 14:12:17.450975   71032 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0508 14:12:17.573303   71032 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0508 14:12:17.573514   71032 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0508 14:12:17.573544   71032 cli_runner.go:164] Run: docker network inspect minikube
W0508 14:12:17.656293   71032 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0508 14:12:17.656344   71032 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0508 14:12:17.656661   71032 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0508 14:12:17.657094   71032 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0508 14:12:17.772847   71032 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x1400294c5a0}
I0508 14:12:17.773278   71032 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 65535 ...
I0508 14:12:17.773414   71032 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0508 14:12:17.972892   71032 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0508 14:12:17.973582   71032 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0508 14:12:17.973771   71032 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0508 14:12:18.100991   71032 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0508 14:12:18.222680   71032 oci.go:103] Successfully created a docker volume minikube
I0508 14:12:18.222896   71032 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib
I0508 14:12:21.241489   71032 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib: (3.018490458s)
I0508 14:12:21.241661   71032 oci.go:107] Successfully prepared a docker volume minikube
I0508 14:12:21.242018   71032 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0508 14:12:21.245070   71032 kic.go:194] Starting extracting preloaded images to volume ...
I0508 14:12:21.245752   71032 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/saad/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir
I0508 14:12:40.935185   71032 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/saad/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir: (19.689193584s)
I0508 14:12:40.936042   71032 kic.go:203] duration metric: took 19.691436s to extract preloaded images to volume ...
I0508 14:12:40.937827   71032 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0508 14:12:42.267717   71032 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (1.329837667s)
I0508 14:12:42.269404   71032 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737
I0508 14:12:43.890109   71032 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737: (1.620564625s)
I0508 14:12:43.893384   71032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0508 14:12:44.019318   71032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 14:12:44.127139   71032 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0508 14:12:44.444676   71032 oci.go:144] the created container "minikube" has a running status.
I0508 14:12:44.444750   71032 kic.go:225] Creating ssh key for kic: /Users/saad/.minikube/machines/minikube/id_rsa...
I0508 14:12:45.147369   71032 kic_runner.go:191] docker (temp): /Users/saad/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0508 14:12:45.402241   71032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 14:12:45.523675   71032 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0508 14:12:45.523701   71032 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0508 14:12:45.806495   71032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 14:12:45.912178   71032 machine.go:94] provisionDockerMachine start ...
I0508 14:12:45.918264   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:12:46.030316   71032 main.go:141] libmachine: Using SSH client type: native
I0508 14:12:46.047410   71032 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102ef7280] 0x102ef9ae0 <nil>  [] 0s} 127.0.0.1 49396 <nil> <nil>}
I0508 14:12:46.047424   71032 main.go:141] libmachine: About to run SSH command:
hostname
I0508 14:12:46.369973   71032 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0508 14:12:46.370654   71032 ubuntu.go:169] provisioning hostname "minikube"
I0508 14:12:46.374651   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:12:46.485068   71032 main.go:141] libmachine: Using SSH client type: native
I0508 14:12:46.485424   71032 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102ef7280] 0x102ef9ae0 <nil>  [] 0s} 127.0.0.1 49396 <nil> <nil>}
I0508 14:12:46.485436   71032 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0508 14:12:46.781801   71032 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0508 14:12:46.783998   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:12:46.904533   71032 main.go:141] libmachine: Using SSH client type: native
I0508 14:12:46.907403   71032 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102ef7280] 0x102ef9ae0 <nil>  [] 0s} 127.0.0.1 49396 <nil> <nil>}
I0508 14:12:46.907423   71032 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0508 14:12:47.162612   71032 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0508 14:12:47.162635   71032 ubuntu.go:175] set auth options {CertDir:/Users/saad/.minikube CaCertPath:/Users/saad/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/saad/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/saad/.minikube/machines/server.pem ServerKeyPath:/Users/saad/.minikube/machines/server-key.pem ClientKeyPath:/Users/saad/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/saad/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/saad/.minikube}
I0508 14:12:47.162665   71032 ubuntu.go:177] setting up certificates
I0508 14:12:47.164486   71032 provision.go:84] configureAuth start
I0508 14:12:47.166430   71032 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0508 14:12:47.296324   71032 provision.go:143] copyHostCerts
I0508 14:12:47.297148   71032 exec_runner.go:151] cp: /Users/saad/.minikube/certs/cert.pem --> /Users/saad/.minikube/cert.pem (1115 bytes)
I0508 14:12:47.299430   71032 exec_runner.go:151] cp: /Users/saad/.minikube/certs/key.pem --> /Users/saad/.minikube/key.pem (1675 bytes)
I0508 14:12:47.303085   71032 exec_runner.go:151] cp: /Users/saad/.minikube/certs/ca.pem --> /Users/saad/.minikube/ca.pem (1074 bytes)
I0508 14:12:47.308163   71032 provision.go:117] generating server cert: /Users/saad/.minikube/machines/server.pem ca-key=/Users/saad/.minikube/certs/ca.pem private-key=/Users/saad/.minikube/certs/ca-key.pem org=saad.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0508 14:12:48.013136   71032 provision.go:177] copyRemoteCerts
I0508 14:12:48.016930   71032 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0508 14:12:48.017055   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:12:48.123796   71032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49396 SSHKeyPath:/Users/saad/.minikube/machines/minikube/id_rsa Username:docker}
I0508 14:12:48.260396   71032 ssh_runner.go:362] scp /Users/saad/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0508 14:12:48.327780   71032 ssh_runner.go:362] scp /Users/saad/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0508 14:12:48.402619   71032 ssh_runner.go:362] scp /Users/saad/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0508 14:12:48.470167   71032 provision.go:87] duration metric: took 1.305694125s to configureAuth
I0508 14:12:48.470189   71032 ubuntu.go:193] setting minikube options for container-runtime
I0508 14:12:48.473248   71032 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0508 14:12:48.473451   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:12:48.575953   71032 main.go:141] libmachine: Using SSH client type: native
I0508 14:12:48.576253   71032 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102ef7280] 0x102ef9ae0 <nil>  [] 0s} 127.0.0.1 49396 <nil> <nil>}
I0508 14:12:48.576275   71032 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0508 14:12:48.780209   71032 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0508 14:12:48.780225   71032 ubuntu.go:71] root file system type: overlay
I0508 14:12:48.781706   71032 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0508 14:12:48.781884   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:12:48.888621   71032 main.go:141] libmachine: Using SSH client type: native
I0508 14:12:48.888988   71032 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102ef7280] 0x102ef9ae0 <nil>  [] 0s} 127.0.0.1 49396 <nil> <nil>}
I0508 14:12:48.889064   71032 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0508 14:12:49.140545   71032 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0508 14:12:49.141328   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:12:49.240155   71032 main.go:141] libmachine: Using SSH client type: native
I0508 14:12:49.240496   71032 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102ef7280] 0x102ef9ae0 <nil>  [] 0s} 127.0.0.1 49396 <nil> <nil>}
I0508 14:12:49.240512   71032 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0508 14:12:50.674235   71032 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-11 10:51:51.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-05-08 09:12:49.160693013 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0508 14:12:50.674262   71032 machine.go:97] duration metric: took 4.762163458s to provisionDockerMachine
I0508 14:12:50.674273   71032 client.go:171] duration metric: took 33.770406208s to LocalClient.Create
I0508 14:12:50.674318   71032 start.go:167] duration metric: took 33.7710575s to libmachine.API.Create "minikube"
I0508 14:12:50.674329   71032 start.go:293] postStartSetup for "minikube" (driver="docker")
I0508 14:12:50.674867   71032 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0508 14:12:50.675087   71032 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0508 14:12:50.675175   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:12:50.751468   71032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49396 SSHKeyPath:/Users/saad/.minikube/machines/minikube/id_rsa Username:docker}
I0508 14:12:50.871762   71032 ssh_runner.go:195] Run: cat /etc/os-release
I0508 14:12:50.886629   71032 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0508 14:12:50.886655   71032 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0508 14:12:50.886662   71032 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0508 14:12:50.886668   71032 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0508 14:12:50.888356   71032 filesync.go:126] Scanning /Users/saad/.minikube/addons for local assets ...
I0508 14:12:50.890182   71032 filesync.go:126] Scanning /Users/saad/.minikube/files for local assets ...
I0508 14:12:50.890253   71032 start.go:296] duration metric: took 215.9255ms for postStartSetup
I0508 14:12:50.897018   71032 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0508 14:12:50.980790   71032 profile.go:143] Saving config to /Users/saad/.minikube/profiles/minikube/config.json ...
I0508 14:12:50.983104   71032 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0508 14:12:50.983191   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:12:51.053219   71032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49396 SSHKeyPath:/Users/saad/.minikube/machines/minikube/id_rsa Username:docker}
I0508 14:12:51.191275   71032 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0508 14:12:51.202135   71032 start.go:128] duration metric: took 34.315501959s to createHost
I0508 14:12:51.202173   71032 start.go:83] releasing machines lock for "minikube", held for 34.316339166s
I0508 14:12:51.202314   71032 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0508 14:12:51.333815   71032 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0508 14:12:51.333915   71032 ssh_runner.go:195] Run: cat /version.json
I0508 14:12:51.334049   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:12:51.336276   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:12:51.474741   71032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49396 SSHKeyPath:/Users/saad/.minikube/machines/minikube/id_rsa Username:docker}
I0508 14:12:51.489077   71032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49396 SSHKeyPath:/Users/saad/.minikube/machines/minikube/id_rsa Username:docker}
I0508 14:12:51.648455   71032 ssh_runner.go:195] Run: systemctl --version
I0508 14:12:53.670780   71032 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.336937458s)
I0508 14:12:53.670837   71032 ssh_runner.go:235] Completed: systemctl --version: (2.022411333s)
W0508 14:12:53.670840   71032 start.go:860] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Operation timed out after 2000 milliseconds with 0 bytes received
I0508 14:12:53.674057   71032 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0508 14:12:53.674297   71032 out.go:239] ❗  This container is having trouble accessing https://registry.k8s.io
W0508 14:12:53.674778   71032 out.go:239] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0508 14:12:53.689059   71032 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0508 14:12:53.728038   71032 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0508 14:12:53.728169   71032 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0508 14:12:54.302078   71032 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0508 14:12:54.302106   71032 start.go:494] detecting cgroup driver to use...
I0508 14:12:54.302146   71032 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0508 14:12:54.305383   71032 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0508 14:12:54.341989   71032 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0508 14:12:54.357725   71032 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0508 14:12:54.373643   71032 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0508 14:12:54.373801   71032 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0508 14:12:54.450003   71032 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0508 14:12:54.470815   71032 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0508 14:12:54.487982   71032 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0508 14:12:54.509767   71032 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0508 14:12:54.527219   71032 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0508 14:12:54.541922   71032 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0508 14:12:54.557901   71032 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0508 14:12:54.572455   71032 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0508 14:12:54.584581   71032 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0508 14:12:54.603611   71032 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0508 14:12:54.710019   71032 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0508 14:12:54.891915   71032 start.go:494] detecting cgroup driver to use...
I0508 14:12:54.891989   71032 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0508 14:12:54.892152   71032 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0508 14:12:54.915980   71032 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0508 14:12:54.918622   71032 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0508 14:12:54.939409   71032 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0508 14:12:54.966317   71032 ssh_runner.go:195] Run: which cri-dockerd
I0508 14:12:54.974099   71032 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0508 14:12:54.989844   71032 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0508 14:12:55.046985   71032 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0508 14:12:55.181328   71032 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0508 14:12:55.299287   71032 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0508 14:12:55.309090   71032 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0508 14:12:55.334959   71032 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0508 14:12:55.417149   71032 ssh_runner.go:195] Run: sudo systemctl restart docker
I0508 14:12:56.166891   71032 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0508 14:12:56.187604   71032 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0508 14:12:56.203914   71032 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0508 14:12:56.297321   71032 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0508 14:12:56.386464   71032 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0508 14:12:56.474328   71032 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0508 14:12:56.505838   71032 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0508 14:12:56.522412   71032 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0508 14:12:56.611877   71032 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0508 14:12:56.854436   71032 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0508 14:12:56.857760   71032 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0508 14:12:56.865772   71032 start.go:562] Will wait 60s for crictl version
I0508 14:12:56.865920   71032 ssh_runner.go:195] Run: which crictl
I0508 14:12:56.872226   71032 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0508 14:12:56.972121   71032 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0508 14:12:56.972259   71032 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0508 14:12:57.033045   71032 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0508 14:12:57.083166   71032 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0508 14:12:57.085271   71032 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0508 14:12:57.308934   71032 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0508 14:12:57.310720   71032 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0508 14:12:57.320208   71032 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0508 14:12:57.337923   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0508 14:12:57.415550   71032 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0508 14:12:57.429433   71032 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0508 14:12:57.431456   71032 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0508 14:12:57.462386   71032 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0508 14:12:57.462397   71032 docker.go:615] Images already preloaded, skipping extraction
I0508 14:12:57.465495   71032 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0508 14:12:57.507811   71032 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0508 14:12:57.507839   71032 cache_images.go:84] Images are preloaded, skipping loading
I0508 14:12:57.507845   71032 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0508 14:12:57.511890   71032 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0508 14:12:57.513859   71032 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0508 14:12:57.624345   71032 cni.go:84] Creating CNI manager for ""
I0508 14:12:57.624360   71032 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0508 14:12:57.624419   71032 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0508 14:12:57.624433   71032 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0508 14:12:57.626596   71032 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0508 14:12:57.626712   71032 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0508 14:12:57.640323   71032 binaries.go:44] Found k8s binaries, skipping transfer
I0508 14:12:57.640457   71032 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0508 14:12:57.659227   71032 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0508 14:12:57.682942   71032 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0508 14:12:57.707538   71032 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0508 14:12:57.732667   71032 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0508 14:12:57.741752   71032 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0508 14:12:57.759213   71032 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0508 14:12:57.842738   71032 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0508 14:12:57.877268   71032 certs.go:68] Setting up /Users/saad/.minikube/profiles/minikube for IP: 192.168.49.2
I0508 14:12:57.877280   71032 certs.go:194] generating shared ca certs ...
I0508 14:12:57.877296   71032 certs.go:226] acquiring lock for ca certs: {Name:mkea6d19306ec6beb2cb1e850368322afb5ee328 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:12:57.884515   71032 certs.go:240] generating "minikubeCA" ca cert: /Users/saad/.minikube/ca.key
I0508 14:12:58.145909   71032 crypto.go:156] Writing cert to /Users/saad/.minikube/ca.crt ...
I0508 14:12:58.145936   71032 lock.go:35] WriteFile acquiring /Users/saad/.minikube/ca.crt: {Name:mk442551844b2c36d497f01d514acab1875500cf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:12:58.149509   71032 crypto.go:164] Writing key to /Users/saad/.minikube/ca.key ...
I0508 14:12:58.149520   71032 lock.go:35] WriteFile acquiring /Users/saad/.minikube/ca.key: {Name:mk995fd6fc24e0253507bd6715a6ec9e1c268984 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:12:58.149718   71032 certs.go:240] generating "proxyClientCA" ca cert: /Users/saad/.minikube/proxy-client-ca.key
I0508 14:12:58.416551   71032 crypto.go:156] Writing cert to /Users/saad/.minikube/proxy-client-ca.crt ...
I0508 14:12:58.416561   71032 lock.go:35] WriteFile acquiring /Users/saad/.minikube/proxy-client-ca.crt: {Name:mk02ae34d1c0f5ff66d7c039b5c902bbd222712d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:12:58.416903   71032 crypto.go:164] Writing key to /Users/saad/.minikube/proxy-client-ca.key ...
I0508 14:12:58.416908   71032 lock.go:35] WriteFile acquiring /Users/saad/.minikube/proxy-client-ca.key: {Name:mkd49866f061ce2788f78d99cb7a59bf9aca93a1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:12:58.417086   71032 certs.go:256] generating profile certs ...
I0508 14:12:58.417131   71032 certs.go:363] generating signed profile cert for "minikube-user": /Users/saad/.minikube/profiles/minikube/client.key
I0508 14:12:58.418907   71032 crypto.go:68] Generating cert /Users/saad/.minikube/profiles/minikube/client.crt with IP's: []
I0508 14:12:58.562428   71032 crypto.go:156] Writing cert to /Users/saad/.minikube/profiles/minikube/client.crt ...
I0508 14:12:58.562439   71032 lock.go:35] WriteFile acquiring /Users/saad/.minikube/profiles/minikube/client.crt: {Name:mk0ea7f4091173b52f32b5c20b98c840bf8667c5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:12:58.562803   71032 crypto.go:164] Writing key to /Users/saad/.minikube/profiles/minikube/client.key ...
I0508 14:12:58.562807   71032 lock.go:35] WriteFile acquiring /Users/saad/.minikube/profiles/minikube/client.key: {Name:mk1bceac7a32a2f10e35ae274dc4422012fa3c07 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:12:58.562996   71032 certs.go:363] generating signed profile cert for "minikube": /Users/saad/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0508 14:12:58.563009   71032 crypto.go:68] Generating cert /Users/saad/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0508 14:12:58.846197   71032 crypto.go:156] Writing cert to /Users/saad/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0508 14:12:58.846210   71032 lock.go:35] WriteFile acquiring /Users/saad/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk80c7dc931e0ae88803d0cad4450aab1a87f721 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:12:58.846546   71032 crypto.go:164] Writing key to /Users/saad/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0508 14:12:58.846550   71032 lock.go:35] WriteFile acquiring /Users/saad/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mkb22acdd72b62d8d7ad7dad9ade68bda507d75a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:12:58.846740   71032 certs.go:381] copying /Users/saad/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /Users/saad/.minikube/profiles/minikube/apiserver.crt
I0508 14:12:58.848156   71032 certs.go:385] copying /Users/saad/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /Users/saad/.minikube/profiles/minikube/apiserver.key
I0508 14:12:58.849796   71032 certs.go:363] generating signed profile cert for "aggregator": /Users/saad/.minikube/profiles/minikube/proxy-client.key
I0508 14:12:58.849816   71032 crypto.go:68] Generating cert /Users/saad/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0508 14:12:59.034454   71032 crypto.go:156] Writing cert to /Users/saad/.minikube/profiles/minikube/proxy-client.crt ...
I0508 14:12:59.034475   71032 lock.go:35] WriteFile acquiring /Users/saad/.minikube/profiles/minikube/proxy-client.crt: {Name:mk7a480c93a2c84e736a8cfb34e111e100941ced Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:12:59.044315   71032 crypto.go:164] Writing key to /Users/saad/.minikube/profiles/minikube/proxy-client.key ...
I0508 14:12:59.044340   71032 lock.go:35] WriteFile acquiring /Users/saad/.minikube/profiles/minikube/proxy-client.key: {Name:mk0affb0e0270d926c6041aba0c8027fad29fff1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:12:59.048569   71032 certs.go:484] found cert: /Users/saad/.minikube/certs/ca-key.pem (1679 bytes)
I0508 14:12:59.051023   71032 certs.go:484] found cert: /Users/saad/.minikube/certs/ca.pem (1074 bytes)
I0508 14:12:59.052248   71032 certs.go:484] found cert: /Users/saad/.minikube/certs/cert.pem (1115 bytes)
I0508 14:12:59.053078   71032 certs.go:484] found cert: /Users/saad/.minikube/certs/key.pem (1675 bytes)
I0508 14:12:59.096451   71032 ssh_runner.go:362] scp /Users/saad/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0508 14:12:59.129788   71032 ssh_runner.go:362] scp /Users/saad/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0508 14:12:59.159559   71032 ssh_runner.go:362] scp /Users/saad/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0508 14:12:59.188929   71032 ssh_runner.go:362] scp /Users/saad/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0508 14:12:59.217853   71032 ssh_runner.go:362] scp /Users/saad/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0508 14:12:59.247257   71032 ssh_runner.go:362] scp /Users/saad/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0508 14:12:59.276682   71032 ssh_runner.go:362] scp /Users/saad/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0508 14:12:59.305873   71032 ssh_runner.go:362] scp /Users/saad/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0508 14:12:59.334540   71032 ssh_runner.go:362] scp /Users/saad/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0508 14:12:59.363123   71032 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0508 14:12:59.386490   71032 ssh_runner.go:195] Run: openssl version
I0508 14:12:59.397103   71032 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0508 14:12:59.412823   71032 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0508 14:12:59.418340   71032 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  8 09:12 /usr/share/ca-certificates/minikubeCA.pem
I0508 14:12:59.418471   71032 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0508 14:12:59.428136   71032 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0508 14:12:59.440979   71032 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0508 14:12:59.446464   71032 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0508 14:12:59.446550   71032 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0508 14:12:59.446728   71032 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0508 14:12:59.474498   71032 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0508 14:12:59.486159   71032 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0508 14:12:59.498588   71032 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0508 14:12:59.500462   71032 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0508 14:12:59.511828   71032 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0508 14:12:59.511843   71032 kubeadm.go:156] found existing configuration files:

I0508 14:12:59.512005   71032 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0508 14:12:59.523315   71032 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0508 14:12:59.523498   71032 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0508 14:12:59.534402   71032 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0508 14:12:59.545307   71032 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0508 14:12:59.545435   71032 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0508 14:12:59.558325   71032 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0508 14:12:59.569309   71032 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0508 14:12:59.569426   71032 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0508 14:12:59.580407   71032 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0508 14:12:59.591167   71032 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0508 14:12:59.591266   71032 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0508 14:12:59.601647   71032 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0508 14:12:59.791495   71032 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0508 14:12:59.791570   71032 kubeadm.go:309] [preflight] Running pre-flight checks
I0508 14:12:59.954613   71032 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0508 14:12:59.954786   71032 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0508 14:12:59.954923   71032 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0508 14:13:00.211909   71032 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0508 14:13:00.230054   71032 out.go:204]     ▪ Generating certificates and keys ...
I0508 14:13:00.230281   71032 kubeadm.go:309] [certs] Using existing ca certificate authority
I0508 14:13:00.230460   71032 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0508 14:13:00.594067   71032 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I0508 14:13:00.746370   71032 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I0508 14:13:00.988210   71032 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I0508 14:13:01.144093   71032 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I0508 14:13:01.205228   71032 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I0508 14:13:01.205617   71032 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0508 14:13:01.432552   71032 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I0508 14:13:01.432716   71032 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0508 14:13:01.730487   71032 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I0508 14:13:02.124410   71032 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I0508 14:13:02.361541   71032 kubeadm.go:309] [certs] Generating "sa" key and public key
I0508 14:13:02.361645   71032 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0508 14:13:02.439811   71032 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0508 14:13:02.786727   71032 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0508 14:13:02.928689   71032 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0508 14:13:03.191026   71032 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0508 14:13:03.281357   71032 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0508 14:13:03.281734   71032 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0508 14:13:03.283844   71032 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0508 14:13:03.292075   71032 out.go:204]     ▪ Booting up control plane ...
I0508 14:13:03.292413   71032 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0508 14:13:03.292633   71032 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0508 14:13:03.292870   71032 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0508 14:13:03.297662   71032 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0508 14:13:03.298215   71032 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0508 14:13:03.298273   71032 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0508 14:13:03.387313   71032 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0508 14:13:03.387423   71032 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0508 14:13:04.395500   71032 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 1.005744125s
I0508 14:13:04.395688   71032 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0508 14:13:09.398388   71032 kubeadm.go:309] [api-check] The API server is healthy after 5.004175669s
I0508 14:13:09.423031   71032 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0508 14:13:09.439343   71032 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0508 14:13:09.453580   71032 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0508 14:13:09.453821   71032 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0508 14:13:09.460661   71032 kubeadm.go:309] [bootstrap-token] Using token: zpwxix.xa881qykb3o1ojpv
I0508 14:13:09.473623   71032 out.go:204]     ▪ Configuring RBAC rules ...
I0508 14:13:09.473978   71032 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0508 14:13:09.479382   71032 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0508 14:13:09.499136   71032 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0508 14:13:09.502685   71032 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0508 14:13:09.504412   71032 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0508 14:13:09.505950   71032 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0508 14:13:09.805651   71032 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0508 14:13:10.220713   71032 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0508 14:13:10.811936   71032 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0508 14:13:10.813365   71032 kubeadm.go:309] 
I0508 14:13:10.813437   71032 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0508 14:13:10.813442   71032 kubeadm.go:309] 
I0508 14:13:10.813547   71032 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0508 14:13:10.813552   71032 kubeadm.go:309] 
I0508 14:13:10.813585   71032 kubeadm.go:309]   mkdir -p $HOME/.kube
I0508 14:13:10.813674   71032 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0508 14:13:10.813750   71032 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0508 14:13:10.813758   71032 kubeadm.go:309] 
I0508 14:13:10.813832   71032 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0508 14:13:10.813839   71032 kubeadm.go:309] 
I0508 14:13:10.813894   71032 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0508 14:13:10.813898   71032 kubeadm.go:309] 
I0508 14:13:10.813985   71032 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0508 14:13:10.814092   71032 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0508 14:13:10.814178   71032 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0508 14:13:10.814183   71032 kubeadm.go:309] 
I0508 14:13:10.814297   71032 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0508 14:13:10.814411   71032 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0508 14:13:10.814418   71032 kubeadm.go:309] 
I0508 14:13:10.814528   71032 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token zpwxix.xa881qykb3o1ojpv \
I0508 14:13:10.814677   71032 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:21d8ab5a6ff419d96cdf03bf3491d4c8136ddafb8cdc1fed10df765965d9f607 \
I0508 14:13:10.814706   71032 kubeadm.go:309] 	--control-plane 
I0508 14:13:10.814711   71032 kubeadm.go:309] 
I0508 14:13:10.814825   71032 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0508 14:13:10.814829   71032 kubeadm.go:309] 
I0508 14:13:10.814946   71032 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token zpwxix.xa881qykb3o1ojpv \
I0508 14:13:10.815071   71032 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:21d8ab5a6ff419d96cdf03bf3491d4c8136ddafb8cdc1fed10df765965d9f607 
I0508 14:13:10.823932   71032 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0508 14:13:10.824090   71032 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0508 14:13:10.824107   71032 cni.go:84] Creating CNI manager for ""
I0508 14:13:10.824128   71032 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0508 14:13:10.841136   71032 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0508 14:13:10.853311   71032 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0508 14:13:10.872102   71032 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0508 14:13:10.897968   71032 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0508 14:13:10.898137   71032 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0508 14:13:10.899072   71032 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_05_08T14_13_10_0700 minikube.k8s.io/version=v1.33.0 minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0508 14:13:10.913164   71032 ops.go:34] apiserver oom_adj: -16
I0508 14:13:11.619800   71032 kubeadm.go:1107] duration metric: took 721.81ms to wait for elevateKubeSystemPrivileges
W0508 14:13:11.619827   71032 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0508 14:13:11.619832   71032 kubeadm.go:393] duration metric: took 12.173579458s to StartCluster
I0508 14:13:11.622233   71032 settings.go:142] acquiring lock: {Name:mk2ee0cc8fe7eea89587dc231468f223d5d57357 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:13:11.622509   71032 settings.go:150] Updating kubeconfig:  /Users/saad/.kube/config
I0508 14:13:11.623103   71032 lock.go:35] WriteFile acquiring /Users/saad/.kube/config: {Name:mkf426107c7228409105ebb9ebd30b9489eb2601 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 14:13:11.623485   71032 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0508 14:13:11.628224   71032 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0508 14:13:11.637504   71032 out.go:177] 🔎  Verifying Kubernetes components...
I0508 14:13:11.626634   71032 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0508 14:13:11.628332   71032 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0508 14:13:11.637552   71032 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0508 14:13:11.637550   71032 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0508 14:13:11.651760   71032 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0508 14:13:11.651826   71032 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0508 14:13:11.652396   71032 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0508 14:13:11.654102   71032 host.go:66] Checking if "minikube" exists ...
I0508 14:13:11.660381   71032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 14:13:11.660484   71032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 14:13:11.750915   71032 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0508 14:13:11.763082   71032 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0508 14:13:11.763096   71032 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0508 14:13:11.763240   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:13:11.771709   71032 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0508 14:13:11.822614   71032 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0508 14:13:11.822644   71032 host.go:66] Checking if "minikube" exists ...
I0508 14:13:11.823109   71032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 14:13:11.830718   71032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49396 SSHKeyPath:/Users/saad/.minikube/machines/minikube/id_rsa Username:docker}
I0508 14:13:11.845135   71032 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0508 14:13:11.893529   71032 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0508 14:13:11.893548   71032 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0508 14:13:11.893649   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 14:13:11.958688   71032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49396 SSHKeyPath:/Users/saad/.minikube/machines/minikube/id_rsa Username:docker}
I0508 14:13:12.067138   71032 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0508 14:13:12.105616   71032 start.go:946] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0508 14:13:12.105762   71032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0508 14:13:12.173333   71032 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0508 14:13:12.174352   71032 api_server.go:52] waiting for apiserver process to appear ...
I0508 14:13:12.174411   71032 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0508 14:13:12.459987   71032 api_server.go:72] duration metric: took 831.744542ms to wait for apiserver process to appear ...
I0508 14:13:12.460004   71032 api_server.go:88] waiting for apiserver healthz status ...
I0508 14:13:12.460031   71032 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49395/healthz ...
I0508 14:13:12.470428   71032 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0508 14:13:12.467069   71032 api_server.go:279] https://127.0.0.1:49395/healthz returned 200:
ok
I0508 14:13:12.478396   71032 addons.go:505] duration metric: took 851.801417ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0508 14:13:12.471802   71032 api_server.go:141] control plane version: v1.30.0
I0508 14:13:12.478455   71032 api_server.go:131] duration metric: took 18.4425ms to wait for apiserver health ...
I0508 14:13:12.478769   71032 system_pods.go:43] waiting for kube-system pods to appear ...
I0508 14:13:12.489282   71032 system_pods.go:59] 5 kube-system pods found
I0508 14:13:12.489304   71032 system_pods.go:61] "etcd-minikube" [c299a680-75eb-4c9a-90ec-236b238bc076] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0508 14:13:12.489312   71032 system_pods.go:61] "kube-apiserver-minikube" [e53b983e-3ea1-4108-8395-c8efe81d137b] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0508 14:13:12.489322   71032 system_pods.go:61] "kube-controller-manager-minikube" [cf6b0967-a60f-4c25-acf7-b78c682074aa] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0508 14:13:12.489330   71032 system_pods.go:61] "kube-scheduler-minikube" [48c574bd-c52b-4b27-b734-606bb7ae470e] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0508 14:13:12.489335   71032 system_pods.go:61] "storage-provisioner" [7979551c-9659-480c-a869-397452dfcf28] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0508 14:13:12.489341   71032 system_pods.go:74] duration metric: took 10.566416ms to wait for pod list to return data ...
I0508 14:13:12.489354   71032 kubeadm.go:576] duration metric: took 861.120083ms to wait for: map[apiserver:true system_pods:true]
I0508 14:13:12.489366   71032 node_conditions.go:102] verifying NodePressure condition ...
I0508 14:13:12.492305   71032 node_conditions.go:122] node storage ephemeral capacity is 153472156Ki
I0508 14:13:12.492335   71032 node_conditions.go:123] node cpu capacity is 8
I0508 14:13:12.492356   71032 node_conditions.go:105] duration metric: took 2.986959ms to run NodePressure ...
I0508 14:13:12.492364   71032 start.go:240] waiting for startup goroutines ...
I0508 14:13:12.611376   71032 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0508 14:13:12.611430   71032 start.go:245] waiting for cluster config update ...
I0508 14:13:12.611463   71032 start.go:254] writing updated cluster config ...
I0508 14:13:12.613434   71032 ssh_runner.go:195] Run: rm -f paused
I0508 14:13:13.863566   71032 start.go:600] kubectl: 1.30.0, cluster: 1.30.0 (minor skew: 0)
I0508 14:13:13.869595   71032 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 08 10:36:52 minikube cri-dockerd[1313]: time="2024-05-08T10:36:52Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: e5f5e14d8b04: Downloading [=======================================>           ]  57.83MB/74.08MB"
May 08 10:37:02 minikube cri-dockerd[1313]: time="2024-05-08T10:37:02Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: e5f5e14d8b04: Extracting [==================>                                ]  26.74MB/74.08MB"
May 08 10:37:06 minikube cri-dockerd[1313]: time="2024-05-08T10:37:06Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: Status: Downloaded newer image for kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
May 08 11:18:33 minikube dockerd[1087]: time="2024-05-08T11:18:33.292644169Z" level=info msg="ignoring event" container=c6b54e1fced9a27eeb4fdbbecd3688e886807e8d40e6233116550c902cd7839d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:18:33 minikube dockerd[1087]: time="2024-05-08T11:18:33.898920128Z" level=info msg="ignoring event" container=921d68bafc45ff367c010c3a7061002ee41495debea55b356d6e291423a58db9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:18:34 minikube dockerd[1087]: time="2024-05-08T11:18:34.587140670Z" level=info msg="ignoring event" container=36db6ad9e04eb7073be4b1172d55cf0bbe523a6032fe8a0cb49e384e090077c2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:18:34 minikube dockerd[1087]: time="2024-05-08T11:18:34.730146920Z" level=info msg="ignoring event" container=139b755945a58ae80217007ce9be0a4ca84bc4101226e1173daf8efce8a7bc33 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:18:34 minikube dockerd[1087]: time="2024-05-08T11:18:34.832739670Z" level=info msg="ignoring event" container=b66b6cfe30b187f20a9688aa4c59a4149ffe2ebece9356a070174c406aa53985 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:18:34 minikube dockerd[1087]: time="2024-05-08T11:18:34.924174253Z" level=info msg="ignoring event" container=14efd1034b6eda8a95498ee77962cae70ce78ce0589e153529666aaefcfc4e0f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:29:26 minikube cri-dockerd[1313]: time="2024-05-08T11:29:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/be623bba1e520776cc3e5d74d42e4faf69418b8743fc3fdd7299a44ad857a14c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 11:29:26 minikube cri-dockerd[1313]: time="2024-05-08T11:29:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f829a65b87c8c90ec36e8a26d88f98ac5b70f3529f9df7752738d65e01d55877/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 11:29:26 minikube cri-dockerd[1313]: time="2024-05-08T11:29:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bba220aa17857a3032c643184de9766394bf6f2336fb1a1b2c672c2e6c7dba12/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 11:29:29 minikube cri-dockerd[1313]: time="2024-05-08T11:29:29Z" level=info msg="Stop pulling image saadriaz999/devops-myapp:latest: Status: Image is up to date for saadriaz999/devops-myapp:latest"
May 08 11:29:33 minikube cri-dockerd[1313]: time="2024-05-08T11:29:33Z" level=info msg="Stop pulling image saadriaz999/devops-myapp:latest: Status: Image is up to date for saadriaz999/devops-myapp:latest"
May 08 11:29:35 minikube cri-dockerd[1313]: time="2024-05-08T11:29:35Z" level=info msg="Stop pulling image qdrant/qdrant:latest: Status: Image is up to date for qdrant/qdrant:latest"
May 08 11:32:24 minikube dockerd[1087]: time="2024-05-08T11:32:24.301136346Z" level=info msg="ignoring event" container=723a4484c7d863f9d405ca76a3629f0b71d13280bbea70d4250ace02b4ce636a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:32:24 minikube dockerd[1087]: time="2024-05-08T11:32:24.804767846Z" level=info msg="ignoring event" container=bba220aa17857a3032c643184de9766394bf6f2336fb1a1b2c672c2e6c7dba12 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:32:25 minikube dockerd[1087]: time="2024-05-08T11:32:25.144356513Z" level=info msg="ignoring event" container=02460fcee635735065286473ac363ed02e2e8baa913a7dcf198ee2b48a610931 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:32:25 minikube dockerd[1087]: time="2024-05-08T11:32:25.228594721Z" level=info msg="ignoring event" container=2aab272a6ad3879cafa746ac03cc0bf0d34f822ef2829370f491760eb7681583 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:32:25 minikube dockerd[1087]: time="2024-05-08T11:32:25.371349471Z" level=info msg="ignoring event" container=be623bba1e520776cc3e5d74d42e4faf69418b8743fc3fdd7299a44ad857a14c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:32:25 minikube dockerd[1087]: time="2024-05-08T11:32:25.432778429Z" level=info msg="ignoring event" container=f829a65b87c8c90ec36e8a26d88f98ac5b70f3529f9df7752738d65e01d55877 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:33:58 minikube cri-dockerd[1313]: time="2024-05-08T11:33:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/db464dc4ec61e47116d12ed15d5d3fa526dcc184c431d0ab3b79fd82fc1851b5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 11:33:58 minikube cri-dockerd[1313]: time="2024-05-08T11:33:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0124cc4b77c068f6ced3c294ece9aa76e005888cf82d900d108f7b41be4e0981/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 11:33:58 minikube cri-dockerd[1313]: time="2024-05-08T11:33:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/94a11a6b176cfecade54b923c89112dc6c868f444f0c67fa116f2a2cc5759bbd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 11:34:00 minikube cri-dockerd[1313]: time="2024-05-08T11:34:00Z" level=info msg="Stop pulling image qdrant/qdrant:latest: Status: Image is up to date for qdrant/qdrant:latest"
May 08 11:34:03 minikube cri-dockerd[1313]: time="2024-05-08T11:34:03Z" level=info msg="Stop pulling image saadriaz999/devops-myapp:latest: Status: Image is up to date for saadriaz999/devops-myapp:latest"
May 08 11:34:07 minikube cri-dockerd[1313]: time="2024-05-08T11:34:07Z" level=info msg="Stop pulling image saadriaz999/devops-myapp:latest: Status: Image is up to date for saadriaz999/devops-myapp:latest"
May 08 11:36:21 minikube dockerd[1087]: time="2024-05-08T11:36:21.833031928Z" level=info msg="ignoring event" container=d035c5e5de6a7ac269c79f3b06342814d34ce59f3428757d5724c20645cc0ef7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:36:22 minikube dockerd[1087]: time="2024-05-08T11:36:22.333534886Z" level=info msg="ignoring event" container=0124cc4b77c068f6ced3c294ece9aa76e005888cf82d900d108f7b41be4e0981 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:36:22 minikube dockerd[1087]: time="2024-05-08T11:36:22.926249261Z" level=info msg="ignoring event" container=f00bcd6326df3faabe5c3dfb3cb1767b56d1786fa6081047bb6a80c2fdf1426a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:36:23 minikube dockerd[1087]: time="2024-05-08T11:36:23.024196970Z" level=info msg="ignoring event" container=627cf7e78722e06b66892091a82db253702eb705ebdaf244e526c960487307c2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:36:23 minikube dockerd[1087]: time="2024-05-08T11:36:23.330107637Z" level=info msg="ignoring event" container=94a11a6b176cfecade54b923c89112dc6c868f444f0c67fa116f2a2cc5759bbd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:36:23 minikube dockerd[1087]: time="2024-05-08T11:36:23.411992470Z" level=info msg="ignoring event" container=db464dc4ec61e47116d12ed15d5d3fa526dcc184c431d0ab3b79fd82fc1851b5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 11:38:06 minikube cri-dockerd[1313]: time="2024-05-08T11:38:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/03cba019bb67200da00909ba3ca3a290ab75b323c802dea0e57a9fd109b3095d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 11:38:06 minikube cri-dockerd[1313]: time="2024-05-08T11:38:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3a8dd58251997f3c992ff32758b1f6f323fda4a0e61b23687457a8794eaa0d58/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 11:38:07 minikube cri-dockerd[1313]: time="2024-05-08T11:38:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0dfe3d89310605bec16dd0418a67a2b8f0f85e19c774ee4d7596215e66e50dbd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 11:38:09 minikube cri-dockerd[1313]: time="2024-05-08T11:38:09Z" level=info msg="Stop pulling image qdrant/qdrant:latest: Status: Image is up to date for qdrant/qdrant:latest"
May 08 11:38:12 minikube cri-dockerd[1313]: time="2024-05-08T11:38:12Z" level=info msg="Stop pulling image saadriaz999/devops-myapp:latest: Status: Image is up to date for saadriaz999/devops-myapp:latest"
May 08 11:38:15 minikube cri-dockerd[1313]: time="2024-05-08T11:38:15Z" level=info msg="Stop pulling image saadriaz999/devops-myapp:latest: Status: Image is up to date for saadriaz999/devops-myapp:latest"
May 08 22:02:08 minikube dockerd[1087]: time="2024-05-08T22:02:08.107920210Z" level=info msg="ignoring event" container=431af2e881632d41edc25214c4276c62178fd6df432996bbc099e9930d926436 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 22:02:08 minikube dockerd[1087]: time="2024-05-08T22:02:08.610071293Z" level=info msg="ignoring event" container=3a8dd58251997f3c992ff32758b1f6f323fda4a0e61b23687457a8794eaa0d58 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 22:02:08 minikube dockerd[1087]: time="2024-05-08T22:02:08.982020877Z" level=info msg="ignoring event" container=3dad4a0e43f00f9d3d248b97e16d10cd12e2177bf1d51c33c5e280f715809070 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 22:02:09 minikube dockerd[1087]: time="2024-05-08T22:02:09.224693752Z" level=info msg="ignoring event" container=86baee24f8ad47876e6622802c5842ab902876b6ca5e396dd98e9f6f6beb5427 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 22:02:09 minikube dockerd[1087]: time="2024-05-08T22:02:09.238391710Z" level=info msg="ignoring event" container=0dfe3d89310605bec16dd0418a67a2b8f0f85e19c774ee4d7596215e66e50dbd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 22:02:09 minikube dockerd[1087]: time="2024-05-08T22:02:09.417242752Z" level=info msg="ignoring event" container=03cba019bb67200da00909ba3ca3a290ab75b323c802dea0e57a9fd109b3095d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 22:20:12 minikube cri-dockerd[1313]: time="2024-05-08T22:20:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9ad8632a4b8688befc3867343388fe65994cecc3a320b1f3b5ba8b039c0eb1c8/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 22:20:12 minikube cri-dockerd[1313]: time="2024-05-08T22:20:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/40cf5c6333335d5ff85518274ccd85d699deed18132fc3a19b0a738a0dde2b42/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 22:20:12 minikube cri-dockerd[1313]: time="2024-05-08T22:20:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b0b825234722d807a7d4b042ee36d66d8ef276534beb7024a844f890ce9a8d5b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 22:20:25 minikube cri-dockerd[1313]: time="2024-05-08T22:20:25Z" level=info msg="Pulling image saadriaz999/devops-myapp:latest: e252c7a4e17b: Downloading [======>                                            ]  8.633MB/61.82MB"
May 08 22:20:35 minikube cri-dockerd[1313]: time="2024-05-08T22:20:35Z" level=info msg="Pulling image saadriaz999/devops-myapp:latest: aa7e9d224624: Downloading [=========>                                         ]  14.54MB/75.7MB"
May 08 22:20:45 minikube cri-dockerd[1313]: time="2024-05-08T22:20:45Z" level=info msg="Pulling image saadriaz999/devops-myapp:latest: e252c7a4e17b: Downloading [==================>                                ]  22.67MB/61.82MB"
May 08 22:20:55 minikube cri-dockerd[1313]: time="2024-05-08T22:20:55Z" level=info msg="Pulling image saadriaz999/devops-myapp:latest: e252c7a4e17b: Downloading [=========================>                         ]  31.86MB/61.82MB"
May 08 22:21:05 minikube cri-dockerd[1313]: time="2024-05-08T22:21:05Z" level=info msg="Pulling image saadriaz999/devops-myapp:latest: e252c7a4e17b: Downloading [==================================>                ]   43.2MB/61.82MB"
May 08 22:21:15 minikube cri-dockerd[1313]: time="2024-05-08T22:21:15Z" level=info msg="Pulling image saadriaz999/devops-myapp:latest: e252c7a4e17b: Downloading [===========================================>       ]  53.41MB/61.82MB"
May 08 22:21:25 minikube cri-dockerd[1313]: time="2024-05-08T22:21:25Z" level=info msg="Pulling image saadriaz999/devops-myapp:latest: e252c7a4e17b: Downloading [================================================>  ]  59.87MB/61.82MB"
May 08 22:21:35 minikube cri-dockerd[1313]: time="2024-05-08T22:21:35Z" level=info msg="Pulling image saadriaz999/devops-myapp:latest: e252c7a4e17b: Extracting [=================================================> ]  61.28MB/61.82MB"
May 08 22:21:45 minikube cri-dockerd[1313]: time="2024-05-08T22:21:45Z" level=info msg="Pulling image saadriaz999/devops-myapp:latest: aa7e9d224624: Downloading [===============================================>   ]  71.22MB/75.7MB"
May 08 22:21:52 minikube cri-dockerd[1313]: time="2024-05-08T22:21:52Z" level=info msg="Stop pulling image saadriaz999/devops-myapp:latest: Status: Downloaded newer image for saadriaz999/devops-myapp:latest"
May 08 22:21:55 minikube cri-dockerd[1313]: time="2024-05-08T22:21:55Z" level=info msg="Stop pulling image saadriaz999/devops-myapp:latest: Status: Image is up to date for saadriaz999/devops-myapp:latest"
May 08 22:21:58 minikube cri-dockerd[1313]: time="2024-05-08T22:21:58Z" level=info msg="Stop pulling image qdrant/qdrant:latest: Status: Image is up to date for qdrant/qdrant:latest"


==> container status <==
CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
5e331c19d7072       qdrant/qdrant@sha256:1a53a4f9c174c1b031a219b0a1987d328bb8a6fe6a0a6b0316ed4a5035f77bad                  30 seconds ago      Running             db                          0                   9ad8632a4b868       qdrant-deployment-7676f8dc6-96hcg
4a508cead78a2       saadriaz999/devops-myapp@sha256:cf139ce5bee8b514cadf1bfbf2445d24d471e8e65d2b4de9e585456956dddb97       33 seconds ago      Running             app                         0                   b0b825234722d       flask-deployment-5d8bbb7597-f9rp8
3f3db480de939       saadriaz999/devops-myapp@sha256:cf139ce5bee8b514cadf1bfbf2445d24d471e8e65d2b4de9e585456956dddb97       36 seconds ago      Running             app                         0                   40cf5c6333335       flask-deployment-5d8bbb7597-lr8xn
1b096cb6cfea0       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93         12 hours ago        Running             kubernetes-dashboard        0                   ea5964cd5b773       kubernetes-dashboard-779776cb65-gbhcv
3e332b3ae65cf       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   12 hours ago        Running             dashboard-metrics-scraper   0                   1702f0576343d       dashboard-metrics-scraper-b5fc48f67-jgj2n
55f2bb564032e       ba04bb24b9575                                                                                          13 hours ago        Running             storage-provisioner         1                   ad12c97462715       storage-provisioner
26e09add82733       cb7eac0b42cc1                                                                                          13 hours ago        Running             kube-proxy                  0                   56bf43e064da9       kube-proxy-qlvl2
e8557b1a4daf1       2437cf7621777                                                                                          13 hours ago        Running             coredns                     0                   30c9ef94777b3       coredns-7db6d8ff4d-v6h6f
1b6597653a286       ba04bb24b9575                                                                                          13 hours ago        Exited              storage-provisioner         0                   ad12c97462715       storage-provisioner
e5bd13655cd83       68feac521c0f1                                                                                          13 hours ago        Running             kube-controller-manager     0                   9baece014c42f       kube-controller-manager-minikube
2ef42a9752bbe       181f57fd3cdb7                                                                                          13 hours ago        Running             kube-apiserver              0                   50caa2133b800       kube-apiserver-minikube
f94b39b12a9aa       547adae34140b                                                                                          13 hours ago        Running             kube-scheduler              0                   4dac2f800e7ca       kube-scheduler-minikube
db4370947991d       014faa467e297                                                                                          13 hours ago        Running             etcd                        0                   7278cc676210c       etcd-minikube


==> coredns [e8557b1a4daf] <==
[INFO] 10.244.0.11:39375 - 52817 "AAAA IN telemetry.qdrant.io. udp 37 false 512" NOERROR qr,rd,ra 37 0.144398s
[INFO] 10.244.0.12:33942 - 1825 "AAAA IN generativelanguage.googleapis.com.default.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.001192292s
[INFO] 10.244.0.12:33942 - 48170 "A IN generativelanguage.googleapis.com.default.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.001409833s
[INFO] 10.244.0.12:33942 - 9517 "AAAA IN generativelanguage.googleapis.com.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000346958s
[INFO] 10.244.0.12:33942 - 16997 "A IN generativelanguage.googleapis.com.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000173541s
[INFO] 10.244.0.12:33942 - 11419 "A IN generativelanguage.googleapis.com.cluster.local. udp 65 false 512" NXDOMAIN qr,aa,rd 158 0.000105167s
[INFO] 10.244.0.12:33942 - 51277 "AAAA IN generativelanguage.googleapis.com.cluster.local. udp 65 false 512" NXDOMAIN qr,aa,rd 158 0.000183458s
[INFO] 10.244.0.12:33942 - 857 "AAAA IN generativelanguage.googleapis.com. udp 51 false 512" NOERROR qr,rd,ra 295 0.011130125s
[INFO] 10.244.0.12:33942 - 5879 "A IN generativelanguage.googleapis.com. udp 51 false 512" NOERROR qr,rd,ra 492 0.011437s
[INFO] 10.244.0.12:56780 - 36449 "SRV IN _grpclb._tcp.generativelanguage.googleapis.com. udp 64 false 512" NOERROR qr,rd,ra 64 0.313526334s
[INFO] 10.244.0.12:40731 - 46368 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.001643542s
[INFO] 10.244.0.12:40731 - 22820 "A IN db.default.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.001985834s
[INFO] 10.244.0.12:34522 - 60458 "A IN db.svc.cluster.local. udp 38 false 512" NXDOMAIN qr,aa,rd 131 0.000138542s
[INFO] 10.244.0.12:34522 - 6423 "AAAA IN db.svc.cluster.local. udp 38 false 512" NXDOMAIN qr,aa,rd 131 0.000413625s
[INFO] 10.244.0.12:49971 - 62913 "AAAA IN db.cluster.local. udp 34 false 512" NXDOMAIN qr,aa,rd 127 0.000112667s
[INFO] 10.244.0.12:49971 - 63455 "A IN db.cluster.local. udp 34 false 512" NXDOMAIN qr,aa,rd 127 0.00005325s
[INFO] 10.244.0.12:50861 - 42097 "A IN db. udp 20 false 512" NXDOMAIN qr,rd,ra 20 0.027587667s
[INFO] 10.244.0.12:50861 - 37491 "AAAA IN db. udp 20 false 512" NXDOMAIN qr,rd,ra 20 0.028035125s
[INFO] 10.244.0.15:48752 - 33489 "AAAA IN telemetry.qdrant.io.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.004504833s
[INFO] 10.244.0.15:48752 - 23748 "A IN telemetry.qdrant.io.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.004640792s
[INFO] 10.244.0.15:59070 - 3683 "AAAA IN telemetry.qdrant.io.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000100667s
[INFO] 10.244.0.15:59070 - 18286 "A IN telemetry.qdrant.io.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000134042s
[INFO] 10.244.0.15:58152 - 37026 "AAAA IN telemetry.qdrant.io.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000103125s
[INFO] 10.244.0.15:58152 - 8865 "A IN telemetry.qdrant.io.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.00016325s
[INFO] 10.244.0.15:37258 - 18250 "AAAA IN telemetry.qdrant.io. udp 37 false 512" NOERROR qr,rd,ra 37 0.06282825s
[INFO] 10.244.0.15:37258 - 26441 "A IN telemetry.qdrant.io. udp 37 false 512" NOERROR qr,rd,ra 107 0.062835583s
[INFO] 10.244.0.16:45622 - 30412 "AAAA IN generativelanguage.googleapis.com.default.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.00142875s
[INFO] 10.244.0.16:45622 - 2437 "A IN generativelanguage.googleapis.com.default.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.001514958s
[INFO] 10.244.0.16:45622 - 1277 "AAAA IN generativelanguage.googleapis.com.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000501459s
[INFO] 10.244.0.16:45622 - 17631 "A IN generativelanguage.googleapis.com.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000255042s
[INFO] 10.244.0.16:45622 - 46571 "AAAA IN generativelanguage.googleapis.com.cluster.local. udp 65 false 512" NXDOMAIN qr,aa,rd 158 0.000164s
[INFO] 10.244.0.16:45622 - 63875 "A IN generativelanguage.googleapis.com.cluster.local. udp 65 false 512" NXDOMAIN qr,aa,rd 158 0.000196875s
[INFO] 10.244.0.16:45622 - 658 "A IN generativelanguage.googleapis.com. udp 51 false 512" NOERROR qr,rd,ra 590 0.166674084s
[INFO] 10.244.0.16:45622 - 25829 "AAAA IN generativelanguage.googleapis.com. udp 51 false 512" NOERROR qr,rd,ra 295 0.173882542s
[INFO] 10.244.0.16:35770 - 8943 "SRV IN _grpclb._tcp.generativelanguage.googleapis.com. udp 64 false 512" NOERROR qr,rd,ra 64 0.471093333s
[INFO] 10.244.0.16:39890 - 52358 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000203209s
[INFO] 10.244.0.16:39890 - 36537 "A IN db.default.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000274791s
[INFO] 10.244.0.16:46302 - 62354 "AAAA IN db.svc.cluster.local. udp 38 false 512" NXDOMAIN qr,aa,rd 131 0.00012075s
[INFO] 10.244.0.16:46302 - 61613 "A IN db.svc.cluster.local. udp 38 false 512" NXDOMAIN qr,aa,rd 131 0.000172667s
[INFO] 10.244.0.16:45753 - 31309 "A IN db.cluster.local. udp 34 false 512" NXDOMAIN qr,aa,rd 127 0.000097584s
[INFO] 10.244.0.16:45753 - 29775 "AAAA IN db.cluster.local. udp 34 false 512" NXDOMAIN qr,aa,rd 127 0.000173416s
[INFO] 10.244.0.16:48602 - 46472 "A IN db. udp 20 false 512" NXDOMAIN qr,rd,ra 20 0.006114709s
[INFO] 10.244.0.16:48602 - 33927 "AAAA IN db. udp 20 false 512" NXDOMAIN qr,rd,ra 20 0.00623975s
[INFO] 10.244.0.16:60631 - 30209 "AAAA IN generativelanguage.googleapis.com.default.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.001429541s
[INFO] 10.244.0.16:60631 - 20548 "A IN generativelanguage.googleapis.com.default.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.001376s
[INFO] 10.244.0.16:58322 - 28632 "SRV IN _grpclb._tcp.generativelanguage.googleapis.com. udp 64 false 512" NOERROR qr,rd,ra 64 0.484054125s
[INFO] 10.244.0.16:60631 - 31590 "A IN generativelanguage.googleapis.com.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000632458s
[INFO] 10.244.0.16:60631 - 55450 "AAAA IN generativelanguage.googleapis.com.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0008415s
[INFO] 10.244.0.16:60631 - 56190 "AAAA IN generativelanguage.googleapis.com.cluster.local. udp 65 false 512" NXDOMAIN qr,aa,rd 158 0.000602709s
[INFO] 10.244.0.16:60631 - 22825 "A IN generativelanguage.googleapis.com.cluster.local. udp 65 false 512" NXDOMAIN qr,aa,rd 158 0.000881125s
[INFO] 10.244.0.16:60631 - 24365 "AAAA IN generativelanguage.googleapis.com. udp 51 false 512" NOERROR qr,rd,ra 295 0.165451209s
[INFO] 10.244.0.16:60631 - 59923 "A IN generativelanguage.googleapis.com. udp 51 false 512" NOERROR qr,rd,ra 688 0.176274584s
[INFO] 10.244.0.18:34426 - 40639 "AAAA IN telemetry.qdrant.io.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.004751875s
[INFO] 10.244.0.18:34426 - 15499 "A IN telemetry.qdrant.io.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.004887458s
[INFO] 10.244.0.18:51848 - 8998 "A IN telemetry.qdrant.io.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000329708s
[INFO] 10.244.0.18:51848 - 42042 "AAAA IN telemetry.qdrant.io.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000505583s
[INFO] 10.244.0.18:56747 - 49135 "AAAA IN telemetry.qdrant.io.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000085875s
[INFO] 10.244.0.18:56747 - 41965 "A IN telemetry.qdrant.io.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000115042s
[INFO] 10.244.0.18:58374 - 8608 "A IN telemetry.qdrant.io. udp 37 false 512" NOERROR qr,rd,ra 107 0.267069875s
[INFO] 10.244.0.18:58374 - 15527 "AAAA IN telemetry.qdrant.io. udp 37 false 512" NOERROR qr,rd,ra 37 0.395921167s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_05_08T14_13_10_0700
                    minikube.k8s.io/version=v1.33.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 08 May 2024 09:13:07 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 08 May 2024 22:22:28 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 08 May 2024 22:22:03 +0000   Wed, 08 May 2024 09:13:07 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 08 May 2024 22:22:03 +0000   Wed, 08 May 2024 09:13:07 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 08 May 2024 22:22:03 +0000   Wed, 08 May 2024 09:13:07 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 08 May 2024 22:22:03 +0000   Wed, 08 May 2024 09:13:08 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  153472156Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8141724Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  153472156Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8141724Ki
  pods:               110
System Info:
  Machine ID:                 01a55cff5aaf4c27976515e4e421e185
  System UUID:                01a55cff5aaf4c27976515e4e421e185
  Boot ID:                    19d16f11-513e-479f-bc7f-2c1b23aa8978
  Kernel Version:             5.15.49-linuxkit-pr
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://26.0.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     flask-deployment-5d8bbb7597-f9rp8            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m18s
  default                     flask-deployment-5d8bbb7597-lr8xn            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m18s
  default                     qdrant-deployment-7676f8dc6-96hcg            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m18s
  kube-system                 coredns-7db6d8ff4d-v6h6f                     100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     13h
  kube-system                 etcd-minikube                                100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         13h
  kube-system                 kube-apiserver-minikube                      250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         13h
  kube-system                 kube-controller-manager-minikube             200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         13h
  kube-system                 kube-proxy-qlvl2                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         13h
  kube-system                 kube-scheduler-minikube                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         13h
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         13h
  kubernetes-dashboard        dashboard-metrics-scraper-b5fc48f67-jgj2n    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11h
  kubernetes-dashboard        kubernetes-dashboard-779776cb65-gbhcv        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[May 8 17:09] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.013371] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +0.020551] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +3.934541] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.000908] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.544255] grpcfuse: loading out-of-tree module taints kernel.
[May 8 17:15] hrtimer: interrupt took 5866542 ns


==> etcd [db4370947991] <==
{"level":"info","ts":"2024-05-08T10:54:11.680213Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3277756036,"revision":4604,"compact-revision":4363}
{"level":"info","ts":"2024-05-08T10:59:11.673372Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4842}
{"level":"info","ts":"2024-05-08T10:59:11.679604Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":4842,"took":"4.569166ms","hash":1146473446,"current-db-size-bytes":2162688,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":1130496,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-08T10:59:11.679712Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1146473446,"revision":4842,"compact-revision":4604}
{"level":"info","ts":"2024-05-08T11:04:11.672973Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5081}
{"level":"info","ts":"2024-05-08T11:04:11.680616Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5081,"took":"5.183333ms","hash":4263790097,"current-db-size-bytes":2162688,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":1130496,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-08T11:04:11.680745Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4263790097,"revision":5081,"compact-revision":4842}
{"level":"info","ts":"2024-05-08T11:09:11.735726Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5323}
{"level":"info","ts":"2024-05-08T11:09:11.742521Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5323,"took":"5.209417ms","hash":918715685,"current-db-size-bytes":2162688,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":1134592,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-08T11:09:11.742637Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":918715685,"revision":5323,"compact-revision":5081}
{"level":"info","ts":"2024-05-08T11:14:11.737464Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5562}
{"level":"info","ts":"2024-05-08T11:14:11.744038Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5562,"took":"4.858916ms","hash":3157656322,"current-db-size-bytes":2162688,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":1118208,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-08T11:14:11.744173Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3157656322,"revision":5562,"compact-revision":5323}
{"level":"info","ts":"2024-05-08T11:19:11.737278Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5803}
{"level":"info","ts":"2024-05-08T11:19:11.742629Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5803,"took":"3.955417ms","hash":3511768232,"current-db-size-bytes":2162688,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":1409024,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-05-08T11:19:11.742725Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3511768232,"revision":5803,"compact-revision":5562}
{"level":"info","ts":"2024-05-08T11:24:11.738011Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6073}
{"level":"info","ts":"2024-05-08T11:24:11.747729Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":6073,"took":"7.468916ms","hash":3959788582,"current-db-size-bytes":2162688,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":1392640,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-05-08T11:24:11.747881Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3959788582,"revision":6073,"compact-revision":5803}
{"level":"info","ts":"2024-05-08T11:29:11.780724Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6312}
{"level":"info","ts":"2024-05-08T11:29:11.787274Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":6312,"took":"4.999208ms","hash":2501760708,"current-db-size-bytes":2162688,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":1048576,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2024-05-08T11:29:11.787418Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2501760708,"revision":6312,"compact-revision":6073}
{"level":"info","ts":"2024-05-08T11:33:44.715488Z","caller":"traceutil/trace.go:171","msg":"trace[760433089] transaction","detail":"{read_only:false; response_revision:6867; number_of_response:1; }","duration":"143.771708ms","start":"2024-05-08T11:33:44.570743Z","end":"2024-05-08T11:33:44.714514Z","steps":["trace[760433089] 'process raft request'  (duration: 143.113958ms)"],"step_count":1}
{"level":"info","ts":"2024-05-08T11:33:55.926202Z","caller":"traceutil/trace.go:171","msg":"trace[357723798] transaction","detail":"{read_only:false; response_revision:6877; number_of_response:1; }","duration":"156.646833ms","start":"2024-05-08T11:33:55.768122Z","end":"2024-05-08T11:33:55.924769Z","steps":["trace[357723798] 'process raft request'  (duration: 156.133375ms)"],"step_count":1}
{"level":"info","ts":"2024-05-08T11:34:11.805526Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6553}
{"level":"info","ts":"2024-05-08T11:34:11.824642Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":6553,"took":"16.087875ms","hash":2141494068,"current-db-size-bytes":2469888,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":2027520,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-08T11:34:11.825297Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2141494068,"revision":6553,"compact-revision":6312}
{"level":"info","ts":"2024-05-08T11:37:38.349028Z","caller":"traceutil/trace.go:171","msg":"trace[1308998979] transaction","detail":"{read_only:false; response_revision:7151; number_of_response:1; }","duration":"549.012125ms","start":"2024-05-08T11:37:37.798998Z","end":"2024-05-08T11:37:38.348011Z","steps":["trace[1308998979] 'process raft request'  (duration: 547.918459ms)"],"step_count":1}
{"level":"info","ts":"2024-05-08T11:37:38.364211Z","caller":"traceutil/trace.go:171","msg":"trace[456344659] transaction","detail":"{read_only:false; response_revision:7152; number_of_response:1; }","duration":"558.297792ms","start":"2024-05-08T11:37:37.805873Z","end":"2024-05-08T11:37:38.364171Z","steps":["trace[456344659] 'process raft request'  (duration: 556.824209ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-08T11:37:38.369136Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-08T11:37:37.805845Z","time spent":"558.451001ms","remote":"127.0.0.1:54718","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:7143 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2024-05-08T11:37:38.369034Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-08T11:37:37.798617Z","time spent":"552.910125ms","remote":"127.0.0.1:54598","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:7150 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-05-08T11:39:11.810935Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6956}
{"level":"info","ts":"2024-05-08T11:39:11.826589Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":6956,"took":"13.648042ms","hash":89502066,"current-db-size-bytes":3055616,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":2871296,"current-db-size-in-use":"2.9 MB"}
{"level":"info","ts":"2024-05-08T11:39:11.826653Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":89502066,"revision":6956,"compact-revision":6553}
{"level":"info","ts":"2024-05-08T17:59:06.181385Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7295}
{"level":"info","ts":"2024-05-08T17:59:06.195915Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":7295,"took":"12.209791ms","hash":1826956289,"current-db-size-bytes":3055616,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":2023424,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-05-08T17:59:06.195983Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1826956289,"revision":7295,"compact-revision":6956}
{"level":"warn","ts":"2024-05-08T22:00:13.490169Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.887ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-08T22:00:13.592034Z","caller":"traceutil/trace.go:171","msg":"trace[1735620125] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:7717; }","duration":"292.020708ms","start":"2024-05-08T22:00:13.299332Z","end":"2024-05-08T22:00:13.591353Z","steps":["trace[1735620125] 'assemble the response'  (duration: 78.689041ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-08T22:00:13.593019Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-08T22:00:12.50601Z","time spent":"1.086962375s","remote":"127.0.0.1:54420","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-05-08T22:01:18.315966Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7532}
{"level":"info","ts":"2024-05-08T22:01:18.319254Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":7532,"took":"2.277708ms","hash":3317724811,"current-db-size-bytes":3055616,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1163264,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2024-05-08T22:01:18.319296Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3317724811,"revision":7532,"compact-revision":7295}
{"level":"warn","ts":"2024-05-08T22:02:07.190761Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.600375ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/flask-service\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-08T22:02:07.192919Z","caller":"traceutil/trace.go:171","msg":"trace[1186921882] range","detail":"{range_begin:/registry/services/endpoints/default/flask-service; range_end:; response_count:0; response_revision:7812; }","duration":"102.933167ms","start":"2024-05-08T22:02:07.089805Z","end":"2024-05-08T22:02:07.192738Z","steps":["trace[1186921882] 'agreement among raft nodes before linearized reading'  (duration: 99.8895ms)"],"step_count":1}
{"level":"info","ts":"2024-05-08T22:06:18.348438Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7769}
{"level":"info","ts":"2024-05-08T22:06:18.358126Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":7769,"took":"6.158042ms","hash":496020243,"current-db-size-bytes":3055616,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1458176,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-08T22:06:18.358283Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":496020243,"revision":7769,"compact-revision":7532}
{"level":"info","ts":"2024-05-08T22:07:58.678816Z","caller":"etcdserver/server.go:1401","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-05-08T22:07:58.696542Z","caller":"etcdserver/server.go:2420","msg":"saved snapshot","snapshot-index":10001}
{"level":"info","ts":"2024-05-08T22:07:58.697371Z","caller":"etcdserver/server.go:2450","msg":"compacted Raft logs","compact-index":5001}
{"level":"info","ts":"2024-05-08T22:11:18.352647Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8041}
{"level":"info","ts":"2024-05-08T22:11:18.362878Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":8041,"took":"7.760458ms","hash":242441050,"current-db-size-bytes":3055616,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1576960,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-05-08T22:11:18.362947Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":242441050,"revision":8041,"compact-revision":7769}
{"level":"info","ts":"2024-05-08T22:16:18.354082Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8279}
{"level":"info","ts":"2024-05-08T22:16:18.360492Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":8279,"took":"4.515959ms","hash":1621747672,"current-db-size-bytes":3055616,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1110016,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-08T22:16:18.360609Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1621747672,"revision":8279,"compact-revision":8041}
{"level":"info","ts":"2024-05-08T22:21:18.41289Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8518}
{"level":"info","ts":"2024-05-08T22:21:18.417213Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":8518,"took":"2.984542ms","hash":857686281,"current-db-size-bytes":3055616,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1421312,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-05-08T22:21:18.417288Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":857686281,"revision":8518,"compact-revision":8279}


==> kernel <==
 22:22:29 up  5:13,  0 users,  load average: 1.07, 0.79, 0.64
Linux minikube 5.15.49-linuxkit-pr #1 SMP PREEMPT Thu May 25 07:27:39 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [2ef42a9752bb] <==
I0508 09:13:07.691722       1 shared_informer.go:320] Caches are synced for configmaps
I0508 09:13:07.691739       1 autoregister_controller.go:141] Starting autoregister controller
I0508 09:13:07.691752       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0508 09:13:07.691758       1 cache.go:39] Caches are synced for autoregister controller
I0508 09:13:07.746346       1 shared_informer.go:320] Caches are synced for node_authorizer
I0508 09:13:07.746562       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0508 09:13:07.746581       1 policy_source.go:224] refreshing policies
I0508 09:13:07.749834       1 controller.go:615] quota admission added evaluator for: namespaces
E0508 09:13:07.750004       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0508 09:13:07.952261       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0508 09:13:08.602527       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0508 09:13:08.610024       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0508 09:13:08.610063       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0508 09:13:08.949593       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0508 09:13:08.971974       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0508 09:13:09.054094       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0508 09:13:09.059093       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0508 09:13:09.059823       1 controller.go:615] quota admission added evaluator for: endpoints
I0508 09:13:09.062183       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0508 09:13:09.663277       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0508 09:13:10.215131       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0508 09:13:10.221459       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0508 09:13:10.229136       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0508 09:13:23.563160       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0508 09:13:23.962708       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0508 10:13:33.756125       1 trace.go:236] Trace[1285871147]: "DeltaFIFO Pop Process" ID:v1.,Depth:19,Reason:slow event handlers blocking the queue (08-May-2024 10:13:33.570) (total time: 173ms):
Trace[1285871147]: [173.07375ms] [173.07375ms] END
I0508 10:29:44.737175       1 alloc.go:330] "allocated clusterIPs" service="default/qdrant-service" clusterIPs={"IPv4":"10.101.160.157"}
I0508 10:29:58.217730       1 alloc.go:330] "allocated clusterIPs" service="default/flask-service" clusterIPs={"IPv4":"10.110.245.168"}
I0508 10:36:00.767640       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs={"IPv4":"10.105.39.236"}
I0508 10:36:00.862414       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs={"IPv4":"10.108.49.13"}
E0508 10:43:02.767985       1 upgradeaware.go:427] Error proxying data from client to backend: write tcp 192.168.49.2:48058->192.168.49.2:10250: write: broken pipe
I0508 11:29:25.770080       1 alloc.go:330] "allocated clusterIPs" service="default/qdrant-service" clusterIPs={"IPv4":"10.102.81.40"}
I0508 11:29:25.893713       1 alloc.go:330] "allocated clusterIPs" service="default/flask-service" clusterIPs={"IPv4":"10.104.153.136"}
I0508 11:33:56.331730       1 alloc.go:330] "allocated clusterIPs" service="default/qdrant-service" clusterIPs={"IPv4":"10.96.94.244"}
I0508 11:33:56.474790       1 alloc.go:330] "allocated clusterIPs" service="default/flask-service" clusterIPs={"IPv4":"10.101.59.193"}
I0508 11:37:38.373578       1 trace.go:236] Trace[1743139129]: "Update" accept:application/json, */*,audit-id:52769db1-7f4d-4210-a8aa-66392e5c2803,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/arm64) kubernetes/$Format,verb:PUT (08-May-2024 11:37:37.737) (total time: 633ms):
Trace[1743139129]: ["GuaranteedUpdate etcd3" audit-id:52769db1-7f4d-4210-a8aa-66392e5c2803,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 623ms (11:37:37.747)
Trace[1743139129]:  ---"Txn call completed" 582ms (11:37:38.370)]
Trace[1743139129]: [633.606209ms] [633.606209ms] END
I0508 11:37:38.373578       1 trace.go:236] Trace[46013856]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ad69b724-d42c-4237-9c9c-14d2a4508e06,client:127.0.0.1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/arm64) kubernetes/7c48c2b,verb:PUT (08-May-2024 11:37:37.739) (total time: 631ms):
Trace[46013856]: ["GuaranteedUpdate etcd3" audit-id:ad69b724-d42c-4237-9c9c-14d2a4508e06,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 628ms (11:37:37.742)
Trace[46013856]:  ---"Txn call completed" 577ms (11:37:38.370)]
Trace[46013856]: [631.940751ms] [631.940751ms] END
I0508 11:38:06.017678       1 alloc.go:330] "allocated clusterIPs" service="default/qdrant-service" clusterIPs={"IPv4":"10.111.33.26"}
I0508 11:38:06.171785       1 alloc.go:330] "allocated clusterIPs" service="default/flask-service" clusterIPs={"IPv4":"10.106.162.18"}
E0508 13:56:37.339868       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0508 13:56:37.339871       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0508 16:58:22.432517       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0508 16:58:22.433229       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0508 19:40:32.491455       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0508 19:40:32.491456       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0508 21:58:39.580935       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0508 21:58:39.580939       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0508 22:02:06.984797       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0508 22:20:10.259410       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0508 22:20:10.283987       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0508 22:20:10.561124       1 alloc.go:330] "allocated clusterIPs" service="default/qdrant-service" clusterIPs={"IPv4":"10.96.208.216"}
E0508 22:20:10.566599       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0508 22:20:10.685275       1 alloc.go:330] "allocated clusterIPs" service="default/flask-service" clusterIPs={"IPv4":"10.106.91.80"}


==> kube-controller-manager [e5bd13655cd8] <==
I0508 11:33:56.125288       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="50.292µs"
I0508 11:33:56.231497       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="96.874208ms"
I0508 11:33:56.233239       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="20.417µs"
I0508 11:33:56.240633       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="8.698917ms"
I0508 11:33:56.242238       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="69.292µs"
I0508 11:33:56.301712       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="91.625µs"
I0508 11:33:56.321012       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="78.958µs"
I0508 11:34:02.448855       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="18.491708ms"
I0508 11:34:02.449156       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="66.833µs"
I0508 11:34:04.716690       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="76.904875ms"
I0508 11:34:04.717080       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="229.709µs"
I0508 11:34:09.200619       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="163.25725ms"
I0508 11:34:09.202587       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="647.375µs"
I0508 11:36:21.404390       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="74.291µs"
I0508 11:36:21.522631       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="33.042µs"
I0508 11:38:05.706850       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="92.370333ms"
I0508 11:38:05.721175       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="14.196833ms"
I0508 11:38:05.721267       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="41.75µs"
I0508 11:38:05.721621       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="18.833µs"
I0508 11:38:05.731861       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="61.5µs"
I0508 11:38:05.896099       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="88.958292ms"
I0508 11:38:05.907279       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="11.051958ms"
I0508 11:38:05.907918       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="43.167µs"
I0508 11:38:05.994026       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="36.083µs"
I0508 11:38:10.022305       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="4.322959ms"
I0508 11:38:10.022413       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="51.583µs"
I0508 11:38:13.128087       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="5.30125ms"
I0508 11:38:13.128535       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="35.834µs"
I0508 11:38:16.297871       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="47.75625ms"
I0508 11:38:16.298103       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="62.542µs"
I0508 13:56:37.343247       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0508 13:56:37.343897       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0508 16:58:22.434498       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0508 16:58:22.434543       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
E0508 19:40:32.494227       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0508 19:40:32.494284       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0508 21:58:39.583284       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0508 21:58:39.583487       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I0508 22:02:07.358416       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="66.959µs"
I0508 22:02:07.480551       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="12µs"
I0508 22:20:10.301280       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="18.474541ms"
E0508 22:20:10.301440       1 replica_set.go:557] sync "default/qdrant-deployment-7676f8dc6" failed with Unauthorized
I0508 22:20:10.316931       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="15.39375ms"
I0508 22:20:10.328130       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="11.113292ms"
I0508 22:20:10.408317       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="80.1285ms"
I0508 22:20:10.408518       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="44.5µs"
I0508 22:20:10.408774       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="54.833µs"
I0508 22:20:10.428184       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="15.0115ms"
I0508 22:20:10.500053       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="71.823458ms"
I0508 22:20:10.500183       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="35.208µs"
I0508 22:20:10.508160       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="47.75µs"
I0508 22:20:10.516657       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="39.459µs"
I0508 22:20:10.567948       1 endpointslice_controller.go:311] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/qdrant-service" err="failed to create EndpointSlice for Service default/qdrant-service: Unauthorized"
I0508 22:20:10.569324       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"qdrant-service", UID:"19b78a27-0f62-46b5-a5c6-1c8dfb29611f", APIVersion:"v1", ResourceVersion:"8741", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/qdrant-service: failed to create EndpointSlice for Service default/qdrant-service: Unauthorized
I0508 22:21:53.809287       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="64.489708ms"
I0508 22:21:53.809544       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="56.958µs"
I0508 22:21:55.799917       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="10.201042ms"
I0508 22:21:55.800048       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-deployment-5d8bbb7597" duration="56.125µs"
I0508 22:21:59.920188       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="8.088709ms"
I0508 22:21:59.920525       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/qdrant-deployment-7676f8dc6" duration="184.458µs"


==> kube-proxy [26e09add8273] <==
I0508 09:13:24.204258       1 server_linux.go:69] "Using iptables proxy"
I0508 09:13:24.213891       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0508 09:13:24.243329       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0508 09:13:24.243365       1 server_linux.go:165] "Using iptables Proxier"
I0508 09:13:24.245959       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0508 09:13:24.245977       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0508 09:13:24.246101       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0508 09:13:24.246515       1 server.go:872] "Version info" version="v1.30.0"
I0508 09:13:24.246557       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0508 09:13:24.247703       1 config.go:192] "Starting service config controller"
I0508 09:13:24.247746       1 shared_informer.go:313] Waiting for caches to sync for service config
I0508 09:13:24.248113       1 config.go:319] "Starting node config controller"
I0508 09:13:24.248171       1 shared_informer.go:313] Waiting for caches to sync for node config
I0508 09:13:24.248281       1 config.go:101] "Starting endpoint slice config controller"
I0508 09:13:24.248336       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0508 09:13:24.348690       1 shared_informer.go:320] Caches are synced for node config
I0508 09:13:24.348726       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0508 09:13:24.348800       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [f94b39b12a9a] <==
I0508 09:13:05.973103       1 serving.go:380] Generated self-signed cert in-memory
W0508 09:13:07.657552       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0508 09:13:07.657574       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0508 09:13:07.657581       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0508 09:13:07.657587       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0508 09:13:07.669140       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0508 09:13:07.669167       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0508 09:13:07.670680       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0508 09:13:07.670727       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0508 09:13:07.670759       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0508 09:13:07.671118       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0508 09:13:07.671954       1 reflector.go:547] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0508 09:13:07.672014       1 reflector.go:150] runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0508 09:13:07.672450       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0508 09:13:07.672471       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0508 09:13:07.672475       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0508 09:13:07.672490       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0508 09:13:07.672534       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0508 09:13:07.672547       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0508 09:13:07.672732       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0508 09:13:07.672756       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0508 09:13:07.673360       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0508 09:13:07.673380       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0508 09:13:07.673368       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0508 09:13:07.673450       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0508 09:13:07.673596       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0508 09:13:07.673617       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0508 09:13:07.673647       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0508 09:13:07.673665       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0508 09:13:07.673652       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0508 09:13:07.673678       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0508 09:13:07.673689       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0508 09:13:07.673685       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0508 09:13:07.673700       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0508 09:13:07.673702       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0508 09:13:07.673744       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0508 09:13:07.673769       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0508 09:13:07.673844       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0508 09:13:07.673859       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0508 09:13:07.673930       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0508 09:13:07.673951       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0508 09:13:08.497808       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0508 09:13:08.497918       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0508 09:13:08.527169       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0508 09:13:08.527257       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0508 09:13:08.687964       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0508 09:13:08.687997       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0508 09:13:08.748188       1 reflector.go:547] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0508 09:13:08.748239       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0508 09:13:08.748255       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0508 09:13:08.748242       1 reflector.go:150] runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0508 09:13:08.751254       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0508 09:13:08.751278       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0508 09:13:08.795556       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0508 09:13:08.795589       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
I0508 09:13:10.471731       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 08 11:36:25 minikube kubelet[2234]: I0508 11:36:25.030616    2234 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="4175a1ae-cd0c-485c-b939-ec8639e306a3" path="/var/lib/kubelet/pods/4175a1ae-cd0c-485c-b939-ec8639e306a3/volumes"
May 08 11:36:25 minikube kubelet[2234]: I0508 11:36:25.031614    2234 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="dd8aadc1-8403-4d8d-ad2f-6ba1b5353787" path="/var/lib/kubelet/pods/dd8aadc1-8403-4d8d-ad2f-6ba1b5353787/volumes"
May 08 11:38:05 minikube kubelet[2234]: I0508 11:38:05.712304    2234 topology_manager.go:215] "Topology Admit Handler" podUID="ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8" podNamespace="default" podName="qdrant-deployment-7676f8dc6-qjp9r"
May 08 11:38:05 minikube kubelet[2234]: E0508 11:38:05.713345    2234 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="3598d27f-91c2-449b-8caa-ce0443120edd" containerName="app"
May 08 11:38:05 minikube kubelet[2234]: E0508 11:38:05.713398    2234 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="dd8aadc1-8403-4d8d-ad2f-6ba1b5353787" containerName="db"
May 08 11:38:05 minikube kubelet[2234]: E0508 11:38:05.713446    2234 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="4175a1ae-cd0c-485c-b939-ec8639e306a3" containerName="app"
May 08 11:38:05 minikube kubelet[2234]: I0508 11:38:05.714022    2234 memory_manager.go:354] "RemoveStaleState removing state" podUID="dd8aadc1-8403-4d8d-ad2f-6ba1b5353787" containerName="db"
May 08 11:38:05 minikube kubelet[2234]: I0508 11:38:05.714051    2234 memory_manager.go:354] "RemoveStaleState removing state" podUID="3598d27f-91c2-449b-8caa-ce0443120edd" containerName="app"
May 08 11:38:05 minikube kubelet[2234]: I0508 11:38:05.714075    2234 memory_manager.go:354] "RemoveStaleState removing state" podUID="4175a1ae-cd0c-485c-b939-ec8639e306a3" containerName="app"
May 08 11:38:05 minikube kubelet[2234]: I0508 11:38:05.719566    2234 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2ds2b\" (UniqueName: \"kubernetes.io/projected/ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8-kube-api-access-2ds2b\") pod \"qdrant-deployment-7676f8dc6-qjp9r\" (UID: \"ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8\") " pod="default/qdrant-deployment-7676f8dc6-qjp9r"
May 08 11:38:05 minikube kubelet[2234]: I0508 11:38:05.822599    2234 topology_manager.go:215] "Topology Admit Handler" podUID="43beab24-6dd7-4b04-8ab1-8f92cf5ced49" podNamespace="default" podName="flask-deployment-5d8bbb7597-ptrff"
May 08 11:38:05 minikube kubelet[2234]: I0508 11:38:05.833117    2234 topology_manager.go:215] "Topology Admit Handler" podUID="c8af891b-c7c4-4a76-93ac-c9100aeadf7c" podNamespace="default" podName="flask-deployment-5d8bbb7597-54zfn"
May 08 11:38:06 minikube kubelet[2234]: I0508 11:38:06.023126    2234 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-pssb6\" (UniqueName: \"kubernetes.io/projected/43beab24-6dd7-4b04-8ab1-8f92cf5ced49-kube-api-access-pssb6\") pod \"flask-deployment-5d8bbb7597-ptrff\" (UID: \"43beab24-6dd7-4b04-8ab1-8f92cf5ced49\") " pod="default/flask-deployment-5d8bbb7597-ptrff"
May 08 11:38:06 minikube kubelet[2234]: I0508 11:38:06.023561    2234 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-q556r\" (UniqueName: \"kubernetes.io/projected/c8af891b-c7c4-4a76-93ac-c9100aeadf7c-kube-api-access-q556r\") pod \"flask-deployment-5d8bbb7597-54zfn\" (UID: \"c8af891b-c7c4-4a76-93ac-c9100aeadf7c\") " pod="default/flask-deployment-5d8bbb7597-54zfn"
May 08 11:38:06 minikube kubelet[2234]: I0508 11:38:06.932740    2234 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3a8dd58251997f3c992ff32758b1f6f323fda4a0e61b23687457a8794eaa0d58"
May 08 11:38:10 minikube kubelet[2234]: I0508 11:38:10.018740    2234 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/qdrant-deployment-7676f8dc6-qjp9r" podStartSLOduration=2.360480754 podStartE2EDuration="5.018219714s" podCreationTimestamp="2024-05-08 11:38:05 +0000 UTC" firstStartedPulling="2024-05-08 11:38:07.197705671 +0000 UTC m=+8032.286612158" lastFinishedPulling="2024-05-08 11:38:09.85544463 +0000 UTC m=+8034.944351118" observedRunningTime="2024-05-08 11:38:10.017921797 +0000 UTC m=+8035.106828326" watchObservedRunningTime="2024-05-08 11:38:10.018219714 +0000 UTC m=+8035.107126285"
May 08 11:38:13 minikube kubelet[2234]: I0508 11:38:13.122941    2234 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/flask-deployment-5d8bbb7597-54zfn" podStartSLOduration=3.024162047 podStartE2EDuration="8.122915757s" podCreationTimestamp="2024-05-08 11:38:05 +0000 UTC" firstStartedPulling="2024-05-08 11:38:07.198102504 +0000 UTC m=+8032.287008992" lastFinishedPulling="2024-05-08 11:38:12.296855965 +0000 UTC m=+8037.385762702" observedRunningTime="2024-05-08 11:38:13.12139334 +0000 UTC m=+8038.210299911" watchObservedRunningTime="2024-05-08 11:38:13.122915757 +0000 UTC m=+8038.211822328"
May 08 11:40:17 minikube kubelet[2234]: I0508 11:40:17.396710    2234 trace.go:236] Trace[714599081]: "iptables ChainExists" (08-May-2024 11:40:15.329) (total time: 2065ms):
May 08 11:40:17 minikube kubelet[2234]: Trace[714599081]: [2.065626292s] [2.065626292s] END
May 08 11:40:17 minikube kubelet[2234]: E0508 11:40:17.606780    2234 kubelet.go:2511] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.412s"
May 08 22:02:07 minikube kubelet[2234]: I0508 22:02:07.367793    2234 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/flask-deployment-5d8bbb7597-ptrff" podStartSLOduration=37434.534325414 podStartE2EDuration="10h24m2.366164459s" podCreationTimestamp="2024-05-08 11:38:05 +0000 UTC" firstStartedPulling="2024-05-08 11:38:07.215147837 +0000 UTC m=+8032.304054325" lastFinishedPulling="2024-05-08 11:38:15.046986758 +0000 UTC m=+8040.135893370" observedRunningTime="2024-05-08 11:38:16.249226258 +0000 UTC m=+8041.338132829" watchObservedRunningTime="2024-05-08 22:02:07.366164459 +0000 UTC m=+8745.966907107"
May 08 22:02:08 minikube kubelet[2234]: I0508 22:02:08.882986    2234 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-2ds2b\" (UniqueName: \"kubernetes.io/projected/ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8-kube-api-access-2ds2b\") pod \"ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8\" (UID: \"ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8\") "
May 08 22:02:08 minikube kubelet[2234]: I0508 22:02:08.893699    2234 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8-kube-api-access-2ds2b" (OuterVolumeSpecName: "kube-api-access-2ds2b") pod "ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8" (UID: "ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8"). InnerVolumeSpecName "kube-api-access-2ds2b". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 08 22:02:08 minikube kubelet[2234]: I0508 22:02:08.984101    2234 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-2ds2b\" (UniqueName: \"kubernetes.io/projected/ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8-kube-api-access-2ds2b\") on node \"minikube\" DevicePath \"\""
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.387425    2234 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-pssb6\" (UniqueName: \"kubernetes.io/projected/43beab24-6dd7-4b04-8ab1-8f92cf5ced49-kube-api-access-pssb6\") pod \"43beab24-6dd7-4b04-8ab1-8f92cf5ced49\" (UID: \"43beab24-6dd7-4b04-8ab1-8f92cf5ced49\") "
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.390745    2234 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/43beab24-6dd7-4b04-8ab1-8f92cf5ced49-kube-api-access-pssb6" (OuterVolumeSpecName: "kube-api-access-pssb6") pod "43beab24-6dd7-4b04-8ab1-8f92cf5ced49" (UID: "43beab24-6dd7-4b04-8ab1-8f92cf5ced49"). InnerVolumeSpecName "kube-api-access-pssb6". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.487859    2234 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-pssb6\" (UniqueName: \"kubernetes.io/projected/43beab24-6dd7-4b04-8ab1-8f92cf5ced49-kube-api-access-pssb6\") on node \"minikube\" DevicePath \"\""
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.590174    2234 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-q556r\" (UniqueName: \"kubernetes.io/projected/c8af891b-c7c4-4a76-93ac-c9100aeadf7c-kube-api-access-q556r\") pod \"c8af891b-c7c4-4a76-93ac-c9100aeadf7c\" (UID: \"c8af891b-c7c4-4a76-93ac-c9100aeadf7c\") "
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.593093    2234 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/c8af891b-c7c4-4a76-93ac-c9100aeadf7c-kube-api-access-q556r" (OuterVolumeSpecName: "kube-api-access-q556r") pod "c8af891b-c7c4-4a76-93ac-c9100aeadf7c" (UID: "c8af891b-c7c4-4a76-93ac-c9100aeadf7c"). InnerVolumeSpecName "kube-api-access-q556r". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.691371    2234 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-q556r\" (UniqueName: \"kubernetes.io/projected/c8af891b-c7c4-4a76-93ac-c9100aeadf7c-kube-api-access-q556r\") on node \"minikube\" DevicePath \"\""
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.739333    2234 scope.go:117] "RemoveContainer" containerID="431af2e881632d41edc25214c4276c62178fd6df432996bbc099e9930d926436"
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.763393    2234 scope.go:117] "RemoveContainer" containerID="3dad4a0e43f00f9d3d248b97e16d10cd12e2177bf1d51c33c5e280f715809070"
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.796645    2234 scope.go:117] "RemoveContainer" containerID="3dad4a0e43f00f9d3d248b97e16d10cd12e2177bf1d51c33c5e280f715809070"
May 08 22:02:09 minikube kubelet[2234]: E0508 22:02:09.798776    2234 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 3dad4a0e43f00f9d3d248b97e16d10cd12e2177bf1d51c33c5e280f715809070" containerID="3dad4a0e43f00f9d3d248b97e16d10cd12e2177bf1d51c33c5e280f715809070"
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.798857    2234 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"3dad4a0e43f00f9d3d248b97e16d10cd12e2177bf1d51c33c5e280f715809070"} err="failed to get container status \"3dad4a0e43f00f9d3d248b97e16d10cd12e2177bf1d51c33c5e280f715809070\": rpc error: code = Unknown desc = Error response from daemon: No such container: 3dad4a0e43f00f9d3d248b97e16d10cd12e2177bf1d51c33c5e280f715809070"
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.798917    2234 scope.go:117] "RemoveContainer" containerID="86baee24f8ad47876e6622802c5842ab902876b6ca5e396dd98e9f6f6beb5427"
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.818025    2234 scope.go:117] "RemoveContainer" containerID="86baee24f8ad47876e6622802c5842ab902876b6ca5e396dd98e9f6f6beb5427"
May 08 22:02:09 minikube kubelet[2234]: E0508 22:02:09.818758    2234 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 86baee24f8ad47876e6622802c5842ab902876b6ca5e396dd98e9f6f6beb5427" containerID="86baee24f8ad47876e6622802c5842ab902876b6ca5e396dd98e9f6f6beb5427"
May 08 22:02:09 minikube kubelet[2234]: I0508 22:02:09.818787    2234 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"86baee24f8ad47876e6622802c5842ab902876b6ca5e396dd98e9f6f6beb5427"} err="failed to get container status \"86baee24f8ad47876e6622802c5842ab902876b6ca5e396dd98e9f6f6beb5427\": rpc error: code = Unknown desc = Error response from daemon: No such container: 86baee24f8ad47876e6622802c5842ab902876b6ca5e396dd98e9f6f6beb5427"
May 08 22:02:11 minikube kubelet[2234]: I0508 22:02:11.507730    2234 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="43beab24-6dd7-4b04-8ab1-8f92cf5ced49" path="/var/lib/kubelet/pods/43beab24-6dd7-4b04-8ab1-8f92cf5ced49/volumes"
May 08 22:02:11 minikube kubelet[2234]: I0508 22:02:11.509198    2234 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="c8af891b-c7c4-4a76-93ac-c9100aeadf7c" path="/var/lib/kubelet/pods/c8af891b-c7c4-4a76-93ac-c9100aeadf7c/volumes"
May 08 22:02:11 minikube kubelet[2234]: I0508 22:02:11.510265    2234 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8" path="/var/lib/kubelet/pods/ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8/volumes"
May 08 22:20:10 minikube kubelet[2234]: I0508 22:20:10.331112    2234 topology_manager.go:215] "Topology Admit Handler" podUID="d2192a0b-adfd-4327-acda-976553c8de3a" podNamespace="default" podName="qdrant-deployment-7676f8dc6-96hcg"
May 08 22:20:10 minikube kubelet[2234]: E0508 22:20:10.333664    2234 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8" containerName="db"
May 08 22:20:10 minikube kubelet[2234]: E0508 22:20:10.333733    2234 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c8af891b-c7c4-4a76-93ac-c9100aeadf7c" containerName="app"
May 08 22:20:10 minikube kubelet[2234]: E0508 22:20:10.333769    2234 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="43beab24-6dd7-4b04-8ab1-8f92cf5ced49" containerName="app"
May 08 22:20:10 minikube kubelet[2234]: I0508 22:20:10.334474    2234 memory_manager.go:354] "RemoveStaleState removing state" podUID="c8af891b-c7c4-4a76-93ac-c9100aeadf7c" containerName="app"
May 08 22:20:10 minikube kubelet[2234]: I0508 22:20:10.334538    2234 memory_manager.go:354] "RemoveStaleState removing state" podUID="43beab24-6dd7-4b04-8ab1-8f92cf5ced49" containerName="app"
May 08 22:20:10 minikube kubelet[2234]: I0508 22:20:10.334555    2234 memory_manager.go:354] "RemoveStaleState removing state" podUID="ecf0d9dd-6e3b-4305-b2cc-0ab1ea86aac8" containerName="db"
May 08 22:20:10 minikube kubelet[2234]: I0508 22:20:10.424962    2234 topology_manager.go:215] "Topology Admit Handler" podUID="b2d3e3ea-5eaa-4119-848b-f1bf88753d51" podNamespace="default" podName="flask-deployment-5d8bbb7597-f9rp8"
May 08 22:20:10 minikube kubelet[2234]: I0508 22:20:10.430536    2234 topology_manager.go:215] "Topology Admit Handler" podUID="196d30df-8a24-474b-a80a-eb1695ecff13" podNamespace="default" podName="flask-deployment-5d8bbb7597-lr8xn"
May 08 22:20:10 minikube kubelet[2234]: I0508 22:20:10.517019    2234 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fqp7h\" (UniqueName: \"kubernetes.io/projected/d2192a0b-adfd-4327-acda-976553c8de3a-kube-api-access-fqp7h\") pod \"qdrant-deployment-7676f8dc6-96hcg\" (UID: \"d2192a0b-adfd-4327-acda-976553c8de3a\") " pod="default/qdrant-deployment-7676f8dc6-96hcg"
May 08 22:20:10 minikube kubelet[2234]: I0508 22:20:10.617717    2234 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6wf8p\" (UniqueName: \"kubernetes.io/projected/196d30df-8a24-474b-a80a-eb1695ecff13-kube-api-access-6wf8p\") pod \"flask-deployment-5d8bbb7597-lr8xn\" (UID: \"196d30df-8a24-474b-a80a-eb1695ecff13\") " pod="default/flask-deployment-5d8bbb7597-lr8xn"
May 08 22:20:10 minikube kubelet[2234]: I0508 22:20:10.618809    2234 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tdjbw\" (UniqueName: \"kubernetes.io/projected/b2d3e3ea-5eaa-4119-848b-f1bf88753d51-kube-api-access-tdjbw\") pod \"flask-deployment-5d8bbb7597-f9rp8\" (UID: \"b2d3e3ea-5eaa-4119-848b-f1bf88753d51\") " pod="default/flask-deployment-5d8bbb7597-f9rp8"
May 08 22:20:12 minikube kubelet[2234]: I0508 22:20:12.217774    2234 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b0b825234722d807a7d4b042ee36d66d8ef276534beb7024a844f890ce9a8d5b"
May 08 22:20:12 minikube kubelet[2234]: I0508 22:20:12.222695    2234 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="9ad8632a4b8688befc3867343388fe65994cecc3a320b1f3b5ba8b039c0eb1c8"
May 08 22:20:12 minikube kubelet[2234]: I0508 22:20:12.229847    2234 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="40cf5c6333335d5ff85518274ccd85d699deed18132fc3a19b0a738a0dde2b42"
May 08 22:21:53 minikube kubelet[2234]: I0508 22:21:53.744362    2234 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/flask-deployment-5d8bbb7597-lr8xn" podStartSLOduration=3.254517338 podStartE2EDuration="1m43.744244467s" podCreationTimestamp="2024-05-08 22:20:10 +0000 UTC" firstStartedPulling="2024-05-08 22:20:12.433857461 +0000 UTC m=+9831.021823143" lastFinishedPulling="2024-05-08 22:21:52.920667216 +0000 UTC m=+9931.511550272" observedRunningTime="2024-05-08 22:21:53.743340259 +0000 UTC m=+9932.334223356" watchObservedRunningTime="2024-05-08 22:21:53.744244467 +0000 UTC m=+9932.335127439"
May 08 22:21:55 minikube kubelet[2234]: I0508 22:21:55.788475    2234 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/flask-deployment-5d8bbb7597-f9rp8" podStartSLOduration=2.86578192 podStartE2EDuration="1m45.788422884s" podCreationTimestamp="2024-05-08 22:20:10 +0000 UTC" firstStartedPulling="2024-05-08 22:20:12.43388142 +0000 UTC m=+9831.021847101" lastFinishedPulling="2024-05-08 22:21:55.353604968 +0000 UTC m=+9933.944488065" observedRunningTime="2024-05-08 22:21:55.788375551 +0000 UTC m=+9934.379258607" watchObservedRunningTime="2024-05-08 22:21:55.788422884 +0000 UTC m=+9934.379305940"
May 08 22:21:59 minikube kubelet[2234]: I0508 22:21:59.906520    2234 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/qdrant-deployment-7676f8dc6-96hcg" podStartSLOduration=3.626446587 podStartE2EDuration="1m49.906293303s" podCreationTimestamp="2024-05-08 22:20:10 +0000 UTC" firstStartedPulling="2024-05-08 22:20:12.43385617 +0000 UTC m=+9831.021821976" lastFinishedPulling="2024-05-08 22:21:58.710785636 +0000 UTC m=+9937.301668692" observedRunningTime="2024-05-08 22:21:59.905620595 +0000 UTC m=+9938.496503609" watchObservedRunningTime="2024-05-08 22:21:59.906293303 +0000 UTC m=+9938.497176317"


==> kubernetes-dashboard [1b096cb6cfea] <==
2024/05/08 22:21:21 received 0 resources from sidecar instead of 3
2024/05/08 22:21:21 [2024-05-08T22:21:21Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/08 22:21:21 Getting list of all pet sets in the cluster
2024/05/08 22:21:21 received 0 resources from sidecar instead of 3
2024/05/08 22:21:21 Getting pod metrics
2024/05/08 22:21:21 received 0 resources from sidecar instead of 3
2024/05/08 22:21:21 [2024-05-08T22:21:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/08 22:21:21 received 0 resources from sidecar instead of 3
2024/05/08 22:21:21 received 0 resources from sidecar instead of 3
2024/05/08 22:21:21 [2024-05-08T22:21:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/08 22:21:21 [2024-05-08T22:21:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/08 22:21:21 received 0 resources from sidecar instead of 3
2024/05/08 22:21:21 Skipping metric because of error: Metric label not set.
2024/05/08 22:21:21 Skipping metric because of error: Metric label not set.
2024/05/08 22:21:21 Skipping metric because of error: Metric label not set.
2024/05/08 22:21:21 Skipping metric because of error: Metric label not set.
2024/05/08 22:21:21 Skipping metric because of error: Metric label not set.
2024/05/08 22:21:21 Skipping metric because of error: Metric label not set.
2024/05/08 22:21:21 [2024-05-08T22:21:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/08 22:21:24 Getting list of all jobs in the cluster
2024/05/08 22:21:24 Getting list of all pods in the cluster
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/05/08 22:21:24 Getting list of namespaces
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/08 22:21:24 Getting list of all cron jobs in the cluster
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/08 22:21:24 Getting list of all deployments in the cluster
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/08 22:21:24 Getting list of all replica sets in the cluster
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/08 22:21:24 Getting list of all replication controllers in the cluster
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/08 22:21:24 Getting list of all pet sets in the cluster
2024/05/08 22:21:24 received 0 resources from sidecar instead of 3
2024/05/08 22:21:24 received 0 resources from sidecar instead of 3
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/08 22:21:24 received 0 resources from sidecar instead of 3
2024/05/08 22:21:24 received 0 resources from sidecar instead of 3
2024/05/08 22:21:24 Getting pod metrics
2024/05/08 22:21:24 received 0 resources from sidecar instead of 3
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/08 22:21:24 received 0 resources from sidecar instead of 3
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/08 22:21:24 received 0 resources from sidecar instead of 3
2024/05/08 22:21:24 received 0 resources from sidecar instead of 3
2024/05/08 22:21:24 Skipping metric because of error: Metric label not set.
2024/05/08 22:21:24 Skipping metric because of error: Metric label not set.
2024/05/08 22:21:24 Skipping metric because of error: Metric label not set.
2024/05/08 22:21:24 Skipping metric because of error: Metric label not set.
2024/05/08 22:21:24 Skipping metric because of error: Metric label not set.
2024/05/08 22:21:24 Skipping metric because of error: Metric label not set.
2024/05/08 22:21:24 [2024-05-08T22:21:24Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [1b6597653a28] <==
I0508 09:13:23.915640       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0508 09:13:53.920159       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [55f2bb564032] <==
I0508 09:13:54.803494       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0508 09:13:54.813728       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0508 09:13:54.813918       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0508 09:13:54.821412       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0508 09:13:54.821604       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_16d60db9-1154-4a75-8e04-63a8665c5839!
I0508 09:13:54.821674       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"3e7ea6b0-4294-4d20-93f9-a23122c534ad", APIVersion:"v1", ResourceVersion:"427", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_16d60db9-1154-4a75-8e04-63a8665c5839 became leader
I0508 09:13:54.921991       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_16d60db9-1154-4a75-8e04-63a8665c5839!

